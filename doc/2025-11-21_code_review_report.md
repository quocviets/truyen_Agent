# B√°o c√°o Review Code - To√†n b·ªô Codebase

**Ng√†y:** 2025-11-21  
**M·ª•c ti√™u:** ƒê√°nh gi√° code quality, maintainability, v√† kh·∫£ nƒÉng update  
**Ph·∫°m vi:** `training/trainer/` - T·∫•t c·∫£ scripts preprocessing, QA, v√† tokenizer

---

## 1. T·ªïng quan

### 1.1. Scripts ƒë√£ review

| Script | M·ª•c ƒë√≠ch | D√≤ng code | Status |
|--------|----------|-----------|--------|
| `preprocessing.py` | Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu raw | ~1,100 | ‚úÖ Ho√†n ch·ªânh |
| `clean_noise.py` | L√†m s·∫°ch noise sau preprocessing | ~250 | ‚úÖ Ho√†n ch·ªânh |
| `data_quality_analysis.py` | QA dataset 8 b∆∞·ªõc | ~400 | ‚úÖ Ho√†n ch·ªânh |
| `split_dataset.py` | Chia train/val/test 90/5/5 | ~370 | ‚úÖ Ho√†n ch·ªânh |
| `build_tokenizer.py` | Build SentencePiece tokenizer | ~330 | ‚úÖ Ho√†n ch·ªânh |

**T·ªïng:** ~2,450 d√≤ng code

---

## 2. ƒêi·ªÉm m·∫°nh (Strengths)

### 2.1. Code Organization ‚úÖ

**T·ªët:**
- ‚úÖ M·ªói script c√≥ m·ªôt m·ª•c ƒë√≠ch r√µ r√†ng (single responsibility)
- ‚úÖ C·∫•u tr√∫c th∆∞ m·ª•c h·ª£p l√Ω: `training/trainer/`, `training/dataset/`, `training/configs/`
- ‚úÖ T√°ch bi·ªát r√µ r√†ng: preprocessing ‚Üí clean ‚Üí QA ‚Üí split ‚Üí tokenizer

**V√≠ d·ª•:**
```python
# preprocessing.py - Ch·ªâ l√†m preprocessing
# clean_noise.py - Ch·ªâ clean noise
# split_dataset.py - Ch·ªâ split dataset
```

### 2.2. Documentation ‚úÖ

**T·ªët:**
- ‚úÖ M·ªói script c√≥ docstring ƒë·∫ßu file m√¥ t·∫£ m·ª•c ƒë√≠ch, usage
- ‚úÖ Functions c√≥ docstrings v·ªõi Args/Returns
- ‚úÖ Comments gi·∫£i th√≠ch logic ph·ª©c t·∫°p
- ‚úÖ Type hints ƒë·∫ßy ƒë·ªß (`from __future__ import annotations`)

**V√≠ d·ª•:**
```python
"""
Build SentencePiece tokenizer t·ª´ train split cho ti·∫øng Vi·ªát.

Chi·∫øn l∆∞·ª£c:
    1. ƒê·ªçc training/dataset/splits/train.jsonl...
    2. Extract text t·ª´ m·ªói paragraph...
"""
```

### 2.3. Error Handling ‚úÖ

**T·ªët:**
- ‚úÖ Try-except cho JSON parsing
- ‚úÖ Ki·ªÉm tra file t·ªìn t·∫°i tr∆∞·ªõc khi ƒë·ªçc
- ‚úÖ Logging warnings cho l·ªói kh√¥ng critical
- ‚úÖ Graceful degradation (b·ªè qua l·ªói, ti·∫øp t·ª•c x·ª≠ l√Ω)

**V√≠ d·ª•:**
```python
try:
    record = json.loads(line)
except json.JSONDecodeError as e:
    print(f"‚ö†Ô∏è  L·ªói parse JSON ·ªü d√≤ng {line_num}: {e}")
    continue
```

### 2.4. CLI Interface ‚úÖ

**T·ªët:**
- ‚úÖ T·∫•t c·∫£ scripts d√πng `argparse` v·ªõi defaults h·ª£p l√Ω
- ‚úÖ Help text r√µ r√†ng
- ‚úÖ Validation cho arguments (choices, type)

**V√≠ d·ª•:**
```python
parser.add_argument(
    "--cleaning-level",
    choices=["safe", "balanced", "aggressive"],
    default="balanced",
    help="M·ª©c ƒë·ªô l√†m s·∫°ch text"
)
```

### 2.5. Reproducibility ‚úÖ

**T·ªët:**
- ‚úÖ Fixed random seeds (42) cho shuffle, sampling
- ‚úÖ Deterministic processing
- ‚úÖ Metadata l∆∞u ƒë·∫ßy ƒë·ªß (summary.json, tokenizer_info.json)

---

## 3. V·∫•n ƒë·ªÅ c·∫ßn c·∫£i thi·ªán (Issues)

### 3.1. ‚ö†Ô∏è Import Path Management

**V·∫•n ƒë·ªÅ:**
- `split_dataset.py` v√† `build_tokenizer.py` d√πng `sys.path.append()` ƒë·ªÉ import `Preprocessor`
- Kh√¥ng c√≥ `__init__.py` trong `training/trainer/` ‚Üí kh√¥ng ph·∫£i package
- Hard-coded path resolution

**Code hi·ªán t·∫°i:**
```python
# split_dataset.py, build_tokenizer.py
import sys
sys.path.append(str(Path(__file__).resolve().parents[2]))
from training.trainer.preprocessing import Preprocessor, CleaningLevel
```

**V·∫•n ƒë·ªÅ:**
- ‚ùå Kh√¥ng portable (ph·ª• thu·ªôc v√†o c·∫•u tr√∫c th∆∞ m·ª•c)
- ‚ùå Kh√≥ test (ph·∫£i setup path ƒë√∫ng)
- ‚ùå Kh√¥ng chu·∫©n Python package structure

**Khuy·∫øn ngh·ªã:**
1. T·∫°o `training/trainer/__init__.py` ƒë·ªÉ bi·∫øn th√†nh package
2. T·∫°o `training/__init__.py` 
3. Ho·∫∑c d√πng relative imports: `from .preprocessing import Preprocessor`

---

### 3.2. ‚ö†Ô∏è Code Duplication

**V·∫•n ƒë·ªÅ:**
- M·ªôt s·ªë logic b·ªã l·∫∑p l·∫°i gi·ªØa c√°c scripts

**V√≠ d·ª• 1: JSONL Reading**
```python
# build_tokenizer.py
with open(input_jsonl, 'r', encoding='utf-8') as f:
    for line_num, line in enumerate(f, 1):
        if not line.strip():
            continue
        try:
            record = json.loads(line)
            text = record.get('text', '').strip()
            if text:
                texts.append(text)
        except json.JSONDecodeError as e:
            print(f"‚ö†Ô∏è  L·ªói parse JSON ·ªü d√≤ng {line_num}: {e}")
            continue
```

```python
# split_dataset.py - C√≥ logic t∆∞∆°ng t·ª±
# clean_noise.py - C√≥ logic t∆∞∆°ng t·ª±
```

**Khuy·∫øn ngh·ªã:**
- T·∫°o utility module `training/trainer/utils.py`:
  ```python
  def read_jsonl(path: Path) -> Iterator[Dict]:
      """Read JSONL file with error handling."""
      ...
  ```

**V√≠ d·ª• 2: Path Constants**
- Nhi·ªÅu scripts hard-code paths nh∆∞ `"training/dataset/preprocessed/..."`

**Khuy·∫øn ngh·ªã:**
- T·∫°o `training/trainer/config.py`:
  ```python
  class Paths:
      RAW_DIR = Path("training/dataset/raw/truyenmoiii_output")
      PREPROCESSED_DIR = Path("training/dataset/preprocessed")
      SPLITS_DIR = Path("training/dataset/splits")
      TOKENIZER_DIR = Path("training/tokenizer")
  ```

---

### 3.3. ‚ö†Ô∏è Configuration Management

**V·∫•n ƒë·ªÅ:**
- Constants r·∫£i r√°c trong code (MIN_PARAGRAPH_LENGTH, MAX_PARAGRAPH_LENGTH, ...)
- Kh√¥ng c√≥ central config file
- Kh√≥ thay ƒë·ªïi config m√† kh√¥ng s·ª≠a code

**V√≠ d·ª•:**
```python
# preprocessing.py
MIN_PARAGRAPH_LENGTH = 50
MAX_PARAGRAPH_LENGTH = 2000
MIN_LINE_LENGTH = 10
```

**Khuy·∫øn ngh·ªã:**
- T·∫°o `training/trainer/config.py`:
  ```python
  @dataclass
  class PreprocessingConfig:
      min_paragraph_length: int = 50
      max_paragraph_length: int = 2000
      min_line_length: int = 10
      cleaning_level: CleaningLevel = CleaningLevel.BALANCED
  ```

---

### 3.4. ‚ö†Ô∏è Logging System

**V·∫•n ƒë·ªÅ:**
- D√πng `print()` thay v√¨ logging module
- Kh√¥ng c√≥ log levels (INFO, WARNING, ERROR)
- Kh√≥ redirect logs v√†o file
- Kh√¥ng c√≥ structured logging

**V√≠ d·ª•:**
```python
print(f"üìñ ƒêang ƒë·ªçc {input_jsonl}...")
print(f"‚úÖ ƒê√£ ƒë·ªçc {len(texts):,} paragraphs")
print(f"‚ö†Ô∏è  L·ªói parse JSON ·ªü d√≤ng {line_num}: {e}")
```

**Khuy·∫øn ngh·ªã:**
- D√πng `logging` module:
  ```python
  import logging
  logger = logging.getLogger(__name__)
  logger.info(f"ƒêang ƒë·ªçc {input_jsonl}...")
  logger.warning(f"L·ªói parse JSON ·ªü d√≤ng {line_num}: {e}")
  ```

---

### 3.5. ‚ö†Ô∏è Windows Encoding Handling

**V·∫•n ƒë·ªÅ:**
- `preprocessing.py` c√≥ code x·ª≠ l√Ω Windows encoding, nh∆∞ng c√°c script kh√°c kh√¥ng c√≥
- Inconsistent

**Code hi·ªán t·∫°i:**
```python
# preprocessing.py
if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except Exception:
        pass
```

**Khuy·∫øn ngh·ªã:**
- T·∫°o utility function trong `utils.py`:
  ```python
  def setup_encoding():
      """Setup UTF-8 encoding for Windows."""
      if sys.platform == 'win32':
          try:
              sys.stdout.reconfigure(encoding='utf-8')
              sys.stderr.reconfigure(encoding='utf-8')
          except Exception:
              pass
  ```

---

### 3.6. ‚ö†Ô∏è Dependencies Management

**V·∫•n ƒë·ªÅ:**
- Ch·ªâ c√≥ `requirements_preprocessing.txt` v·ªõi 2 packages
- Kh√¥ng c√≥ version pinning ch·∫∑t ch·∫Ω
- Kh√¥ng c√≥ `requirements.txt` ·ªü root

**File hi·ªán t·∫°i:**
```
# requirements_preprocessing.txt
tqdm>=4.65.0
sentencepiece>=0.1.99
```

**Khuy·∫øn ngh·ªã:**
- T·∫°o `requirements.txt` ·ªü root v·ªõi t·∫•t c·∫£ dependencies
- Pin versions ch·∫∑t h∆°n (ho·∫∑c d√πng `requirements-dev.txt` cho dev)
- Th√™m `setup.py` ho·∫∑c `pyproject.toml` n·∫øu mu·ªën install nh∆∞ package

---

### 3.7. ‚ö†Ô∏è Testing

**V·∫•n ƒë·ªÅ:**
- ‚ùå Kh√¥ng c√≥ unit tests
- ‚ùå Kh√¥ng c√≥ integration tests
- ‚ùå Kh√¥ng c√≥ test data

**Khuy·∫øn ngh·ªã:**
- T·∫°o `training/trainer/tests/`:
  - `test_preprocessing.py`
  - `test_clean_noise.py`
  - `test_split_dataset.py`
  - `test_build_tokenizer.py`
- D√πng `pytest` ho·∫∑c `unittest`

---

### 3.8. ‚ö†Ô∏è Type Hints Inconsistency

**V·∫•n ƒë·ªÅ:**
- M·ªôt s·ªë functions thi·∫øu return type hints
- M·ªôt s·ªë d√πng `Dict` thay v√¨ `Dict[str, Any]`

**V√≠ d·ª•:**
```python
# data_quality_analysis.py
def load_summary(path: Path) -> Dict:  # N√™n l√† Dict[str, Any]
    ...
```

**Khuy·∫øn ngh·ªã:**
- Th√™m ƒë·∫ßy ƒë·ªß type hints
- D√πng `from typing import Dict, List, Any, Optional, Tuple`

---

## 4. Khuy·∫øn ngh·ªã c·∫£i thi·ªán (Recommendations)

### 4.1. T·∫°o Package Structure

**T·∫°o c√°c file:**
```
training/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ trainer/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py          # NEW
‚îÇ   ‚îú‚îÄ‚îÄ config.py            # NEW - Central config
‚îÇ   ‚îú‚îÄ‚îÄ utils.py             # NEW - Utility functions
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.py
‚îÇ   ‚îú‚îÄ‚îÄ clean_noise.py
‚îÇ   ‚îú‚îÄ‚îÄ data_quality_analysis.py
‚îÇ   ‚îú‚îÄ‚îÄ split_dataset.py
‚îÇ   ‚îú‚îÄ‚îÄ build_tokenizer.py
‚îÇ   ‚îî‚îÄ‚îÄ tests/               # NEW
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ test_preprocessing.py
‚îÇ       ‚îî‚îÄ‚îÄ ...
```

**`training/trainer/__init__.py`:**
```python
"""Training pipeline utilities."""
from .preprocessing import Preprocessor, CleaningLevel
from .config import PreprocessingConfig, Paths

__all__ = [
    'Preprocessor',
    'CleaningLevel',
    'PreprocessingConfig',
    'Paths',
]
```

---

### 4.2. T·∫°o Config Module

**`training/trainer/config.py`:**
```python
"""Central configuration for training pipeline."""
from dataclasses import dataclass
from pathlib import Path
from .preprocessing import CleaningLevel

class Paths:
    """Centralized path constants."""
    ROOT = Path(__file__).resolve().parents[2]
    RAW_DIR = ROOT / "training" / "dataset" / "raw" / "truyenmoiii_output"
    PREPROCESSED_DIR = ROOT / "training" / "dataset" / "preprocessed"
    SPLITS_DIR = ROOT / "training" / "dataset" / "splits"
    TOKENIZER_DIR = ROOT / "training" / "tokenizer"
    MODEL_DIR = ROOT / "training" / "model"

@dataclass
class PreprocessingConfig:
    """Configuration for preprocessing."""
    min_paragraph_length: int = 50
    max_paragraph_length: int = 2000
    min_line_length: int = 10
    cleaning_level: CleaningLevel = CleaningLevel.BALANCED
    min_chapter_length: int = 500
    min_ratio: float = 0.1

@dataclass
class TokenizerConfig:
    """Configuration for tokenizer."""
    vocab_size: int = 32000
    model_type: str = "bpe"
    character_coverage: float = 0.9995
```

---

### 4.3. T·∫°o Utils Module

**`training/trainer/utils.py`:**
```python
"""Utility functions for training pipeline."""
import json
import sys
from pathlib import Path
from typing import Iterator, Dict, Any

def setup_encoding():
    """Setup UTF-8 encoding for Windows."""
    if sys.platform == 'win32':
        try:
            sys.stdout.reconfigure(encoding='utf-8')
            sys.stderr.reconfigure(encoding='utf-8')
        except Exception:
            pass

def read_jsonl(path: Path) -> Iterator[Dict[str, Any]]:
    """Read JSONL file with error handling."""
    with open(path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            if not line.strip():
                continue
            try:
                yield json.loads(line)
            except json.JSONDecodeError as e:
                print(f"‚ö†Ô∏è  L·ªói parse JSON ·ªü d√≤ng {line_num}: {e}")
                continue

def write_jsonl(path: Path, records: Iterator[Dict[str, Any]]):
    """Write records to JSONL file."""
    with open(path, 'w', encoding='utf-8') as f:
        for record in records:
            f.write(json.dumps(record, ensure_ascii=False) + '\n')
```

---

### 4.4. C·∫£i thi·ªán Logging

**T·∫°o `training/trainer/logger.py`:**
```python
"""Logging setup for training pipeline."""
import logging
import sys
from pathlib import Path

def setup_logger(name: str, log_file: Path = None, level=logging.INFO):
    """Setup logger with file and console handlers."""
    logger = logging.getLogger(name)
    logger.setLevel(level)
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(level)
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    # File handler (optional)
    if log_file:
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(level)
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    
    return logger
```

**S·ª≠ d·ª•ng:**
```python
from .logger import setup_logger
logger = setup_logger(__name__)
logger.info("ƒêang ƒë·ªçc file...")
logger.warning("L·ªói parse JSON")
```

---

### 4.5. T·∫°o Requirements File

**`requirements.txt` (root):**
```
# Core dependencies
tqdm>=4.65.0,<5.0.0
sentencepiece>=0.1.99,<1.0.0

# Development dependencies (optional)
pytest>=7.0.0
pytest-cov>=4.0.0
black>=23.0.0
mypy>=1.0.0
```

---

## 5. Action Items (∆Øu ti√™n)

### Priority 1 (Critical - L√†m ngay)

1. **T·∫°o package structure**
   - T·∫°o `training/trainer/__init__.py`
   - T·∫°o `training/__init__.py`
   - Fix imports (b·ªè `sys.path.append`)

2. **T·∫°o config module**
   - `training/trainer/config.py` v·ªõi Paths v√† Config classes
   - Refactor scripts ƒë·ªÉ d√πng config

3. **T·∫°o utils module**
   - `training/trainer/utils.py` v·ªõi common functions
   - Refactor duplicate code

### Priority 2 (Important - L√†m sau)

4. **C·∫£i thi·ªán logging**
   - T·∫°o `logger.py`
   - Thay `print()` b·∫±ng `logger.info/warning/error`

5. **T·∫°o requirements.txt**
   - Root level requirements file
   - Pin versions

### Priority 3 (Nice to have)

6. **T·∫°o tests**
   - Unit tests cho m·ªói module
   - Integration tests

7. **Documentation**
   - API documentation (Sphinx ho·∫∑c mkdocs)
   - Usage examples

---

## 6. Code Quality Metrics

### 6.1. Maintainability Score

| Metric | Score | Notes |
|--------|-------|-------|
| **Code Organization** | 8/10 | T·ªët, nh∆∞ng thi·∫øu package structure |
| **Documentation** | 9/10 | R·∫•t t·ªët, docstrings ƒë·∫ßy ƒë·ªß |
| **Error Handling** | 7/10 | T·ªët, nh∆∞ng thi·∫øu structured logging |
| **Type Hints** | 8/10 | T·ªët, nh∆∞ng m·ªôt s·ªë ch·ªó thi·∫øu |
| **Testing** | 0/10 | ‚ùå Ch∆∞a c√≥ tests |
| **Config Management** | 5/10 | ‚ö†Ô∏è Constants r·∫£i r√°c |
| **Dependencies** | 6/10 | ‚ö†Ô∏è Thi·∫øu requirements.txt ·ªü root |

**Overall: 6.1/10** - T·ªët nh∆∞ng c·∫ßn c·∫£i thi·ªán

---

## 7. K·∫øt lu·∫≠n

### 7.1. ƒêi·ªÉm m·∫°nh

‚úÖ Code organization r√µ r√†ng  
‚úÖ Documentation ƒë·∫ßy ƒë·ªß  
‚úÖ Error handling c∆° b·∫£n t·ªët  
‚úÖ CLI interface chu·∫©n  
‚úÖ Reproducibility t·ªët  

### 7.2. C·∫ßn c·∫£i thi·ªán

‚ö†Ô∏è Package structure (import paths)  
‚ö†Ô∏è Code duplication (utils module)  
‚ö†Ô∏è Config management (central config)  
‚ö†Ô∏è Logging system (structured logging)  
‚ö†Ô∏è Testing (ch∆∞a c√≥ tests)  

### 7.3. Khuy·∫øn ngh·ªã

**Ng·∫Øn h·∫°n (1-2 ng√†y):**
1. T·∫°o package structure
2. T·∫°o config v√† utils modules
3. Refactor imports

**Trung h·∫°n (1 tu·∫ßn):**
4. C·∫£i thi·ªán logging
5. T·∫°o requirements.txt
6. Th√™m type hints ƒë·∫ßy ƒë·ªß

**D√†i h·∫°n (1 th√°ng):**
7. T·∫°o unit tests
8. API documentation
9. CI/CD pipeline

---

**T·ªïng k·∫øt:** Codebase hi·ªán t·∫°i **t·ªët v·ªÅ m·∫∑t functionality** nh∆∞ng c·∫ßn **c·∫£i thi·ªán v·ªÅ m·∫∑t structure v√† maintainability** ƒë·ªÉ d·ªÖ update v√† m·ªü r·ªông sau n√†y.

