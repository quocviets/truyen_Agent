"""
Preprocessing Script - B∆∞·ªõc 1: Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu truy·ªán ti·∫øng Vi·ªát

M·ª•c ƒë√≠ch:
    - ƒê·ªçc d·ªØ li·ªáu raw t·ª´ training/dataset/raw/truyenmoiii_output/
    - L√†m s·∫°ch v√† chu·∫©n h√≥a text theo chi·∫øn l∆∞·ª£c (SAFE/BALANCED/AGGRESSIVE)
    - Filter c√°c chapter qu√° ng·∫Øn ho·∫∑c kh√¥ng h·ª£p l·ªá
    - Chia th√†nh paragraphs h·ª£p l·ªá (50-2000 k√Ω t·ª±)
    - L∆∞u v√†o training/dataset/preprocessed/ (format: txt ho·∫∑c JSONL)

Chi·∫øn l∆∞·ª£c l√†m s·∫°ch:
    - SAFE: Ch·ªâ lo·∫°i b·ªè control chars, chu·∫©n h√≥a whitespace/line breaks
    - BALANCED: SAFE + lo·∫°i b·ªè HTML tags, m·ªôt s·ªë k√Ω t·ª± ƒë·∫∑c bi·ªát (gi·ªØ emoji, k√Ω t·ª± trong t√™n)
    - AGGRESSIVE: BALANCED + lo·∫°i b·ªè emoji, k√Ω t·ª± ƒë·∫∑c bi·ªát

T√°c gi·∫£: AI Agent
Ng√†y: 2024
"""

import os
import re
import json
import unicodedata
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Literal
from tqdm import tqdm

# Import from local modules
from .utils import setup_encoding
from .config import Paths, PreprocessingConfig, CleaningLevel

# Setup encoding for Windows
setup_encoding()


# ============================================================================
# ENUM V√Ä CONSTANTS
# ============================================================================

# Pattern ƒë·ªÉ match s·ªë chapter t·ª´ t√™n file
# V√≠ d·ª•: "chapter_123.txt" ‚Üí 123
CHAPTER_NUMBER_PATTERN = re.compile(r'chapter_(\d+)', re.IGNORECASE)

# Pattern ƒë·ªÉ x√≥a HTML comments (tr∆∞·ªõc khi x·ª≠ l√Ω tags)
# V√≠ d·ª•: "<!-- comment -->" ‚Üí ""
HTML_COMMENT_PATTERN = re.compile(r'<!--.*?-->', re.DOTALL)

# Pattern ƒë·ªÉ chuy·ªÉn <br> v√† <br/> th√†nh newline (tr∆∞·ªõc khi x√≥a tags)
# Gi·ªØ l·∫°i c·∫•u tr√∫c paragraph t·ª´ HTML
BR_TAG_PATTERN = re.compile(r'<br\s*/?>', re.IGNORECASE)

# Pattern ƒë·ªÉ x√≥a HTML tags (sau khi ƒë√£ chuy·ªÉn <br>)
# V√≠ d·ª•: "<strong>text</strong>" ‚Üí "text"
HTML_TAG_PATTERN = re.compile(r'<[^>]+>')

# Pattern ƒë·ªÉ x√≥a control characters (gi·ªØ l·∫°i \n, \t, space)
# \x00-\x08: NULL, SOH, STX, ..., BS
# \x0b-\x0c: VT (vertical tab), FF (form feed)
# \x0e-\x1f: SO, SI, DLE, ..., US
# \x7f-\x9f: DEL, padding, ...
CONTROL_CHARS_PATTERN = re.compile(r'[\x00-\x08\x0b-\x0c\x0e-\x1f\x7f-\x9f]')

# Pattern ƒë·ªÉ normalize whitespace (nhi·ªÅu spaces/tabs ‚Üí 1 space)
# L∆ØU √ù: Ch·ªâ normalize trong d√≤ng, kh√¥ng normalize line breaks
WHITESPACE_PATTERN = re.compile(r'[ \t]+')

# Pattern ƒë·ªÉ normalize line breaks (nhi·ªÅu newlines ‚Üí t·ªëi ƒëa 2)
# Gi·ªØ paragraph structure
MULTIPLE_NEWLINES_PATTERN = re.compile(r'\n{3,}')

# Pattern ƒë·ªÉ chia paragraph (2 newlines li√™n ti·∫øp)
PARAGRAPH_BREAK_PATTERN = re.compile(r'\n\s*\n')

# Pattern ƒë·ªÉ t√°ch c√¢u (d·∫•u ch·∫•m, ch·∫•m h·ªèi, ch·∫•m than)
# D√πng ƒë·ªÉ chia paragraph d√†i
SENTENCE_END_PATTERN = re.compile(r'([.!?]+[\s\n]*)')

# Constants (c√≥ th·ªÉ override b·∫±ng config)
# Gi·ªõi h·∫°n ƒë·ªô d√†i paragraph
MIN_PARAGRAPH_LENGTH = 50      # ƒêo·∫°n qu√° ng·∫Øn (< 50 k√Ω t·ª±) ‚Üí b·ªè qua (TR·ª™ h·ªôi tho·∫°i)
RELAXED_PARAGRAPH_MIN_LENGTH = 30  # Cho ph√©p gi·ªØ ƒëo·∫°n ng·∫Øn 30-49 k√Ω t·ª± n·∫øu c√≥ c√¢u ho√†n ch·ªânh
MAX_PARAGRAPH_LENGTH = 2000     # ƒêo·∫°n qu√° d√†i (> 2000 k√Ω t·ª±) ‚Üí chia nh·ªè

# Gi·ªõi h·∫°n ƒë·ªô d√†i chapter (bytes)
MIN_CHAPTER_LENGTH_BYTES = 500  # Chapter < 500 bytes ‚Üí filter (TR·ª™ n·∫øu c√≥ > 1 paragraph h·ª£p l·ªá)
MIN_CHAPTER_RATIO = 0.1         # Chapter < 10% trung b√¨nh ‚Üí filter (TR·ª™ n·∫øu c√≥ > 1 paragraph h·ª£p l·ªá)

# ƒê·ªô d√†i t·ªëi thi·ªÉu c·ªßa d√≤ng ƒë·ªÉ x√≥a (TR·ª™ h·ªôi tho·∫°i ng·∫Øn)
# H·ªôi tho·∫°i ng·∫Øn th∆∞·ªùng c√≥ d·∫•u c√¢u: . ! ? ... ho·∫∑c d·∫•u ngo·∫∑c k√©p
MIN_LINE_LENGTH = 10
DIALOGUE_PATTERN = re.compile(
    r'^(["\'„Äå„Äé].*[.!?‚Ä¶„ÄÇÔºÅÔºü]|[^.!?]*[.!?‚Ä¶„ÄÇÔºÅÔºü])$'
)


# ============================================================================
# CLASS PREPROCESSOR - X·ª¨ L√ù PREPROCESSING
# ============================================================================

class Preprocessor:
    """
    Class x·ª≠ l√Ω preprocessing d·ªØ li·ªáu truy·ªán ti·∫øng Vi·ªát.
    
    Ch·ª©c nƒÉng ch√≠nh:
        1. ƒê·ªçc d·ªØ li·ªáu raw t·ª´ th∆∞ m·ª•c input
        2. L√†m s·∫°ch text theo chi·∫øn l∆∞·ª£c (SAFE/BALANCED/AGGRESSIVE)
        3. Chia th√†nh paragraphs h·ª£p l·ªá
        4. Filter chapters kh√¥ng h·ª£p l·ªá (c√≥ exception cho chapter c√≥ nhi·ªÅu paragraphs)
        5. L∆∞u k·∫øt qu·∫£ v√†o th∆∞ m·ª•c output (txt ho·∫∑c JSONL)
    
    Attributes:
        raw_dir (Path): Th∆∞ m·ª•c ch·ª©a d·ªØ li·ªáu raw
        output_dir (Path): Th∆∞ m·ª•c output sau preprocessing
        cleaning_level (CleaningLevel): M·ª©c ƒë·ªô l√†m s·∫°ch
        min_chapter_length (int): ƒê·ªô d√†i t·ªëi thi·ªÉu c·ªßa chapter (bytes)
        min_ratio (float): T·ª∑ l·ªá t·ªëi thi·ªÉu so v·ªõi trung b√¨nh (0.1 = 10%)
        stats (Dict): Th·ªëng k√™ qu√° tr√¨nh preprocessing
    """
    
    def __init__(
        self,
        raw_dir: Optional[Path] = None,
        output_dir: Optional[Path] = None,
        cleaning_level: CleaningLevel = CleaningLevel.BALANCED,
        min_chapter_length: int = MIN_CHAPTER_LENGTH_BYTES,
        min_ratio: float = MIN_CHAPTER_RATIO,
        export_global_jsonl: bool = False,
        config: Optional[PreprocessingConfig] = None
    ):
        """
        Kh·ªüi t·∫°o Preprocessor.
        
        Args:
            raw_dir: ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c ch·ª©a d·ªØ li·ªáu raw (default: Paths.RAW_DIR)
            output_dir: ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c output sau preprocessing (default: Paths.PREPROCESSED_DIR)
            cleaning_level: M·ª©c ƒë·ªô l√†m s·∫°ch (SAFE/BALANCED/AGGRESSIVE)
            min_chapter_length: ƒê·ªô d√†i t·ªëi thi·ªÉu c·ªßa chapter (bytes)
            min_ratio: T·ª∑ l·ªá t·ªëi thi·ªÉu so v·ªõi trung b√¨nh (0.1 = 10%)
            export_global_jsonl: True n·∫øu mu·ªën gom t·∫•t c·∫£ paragraph v√†o 1 file JSONL
            config: PreprocessingConfig object (n·∫øu c√≥ s·∫Ω override c√°c tham s·ªë kh√°c)
        
        V√≠ d·ª•:
            >>> preprocessor = Preprocessor(
            ...     cleaning_level=CleaningLevel.BALANCED
            ... )
            >>> # Ho·∫∑c d√πng config
            >>> config = PreprocessingConfig(cleaning_level=CleaningLevel.AGGRESSIVE)
            >>> preprocessor = Preprocessor(config=config)
        """
        # S·ª≠ d·ª•ng config n·∫øu ƒë∆∞·ª£c cung c·∫•p
        if config is not None:
            self.raw_dir = config.raw_dir
            self.output_dir = config.output_dir
            self.cleaning_level = config.cleaning_level
            self.min_chapter_length = config.min_chapter_length
            self.min_ratio = config.min_ratio
        else:
            # S·ª≠ d·ª•ng tham s·ªë ho·∫∑c defaults t·ª´ Paths
            self.raw_dir = Path(raw_dir) if raw_dir is not None else Paths.RAW_DIR
            self.output_dir = Path(output_dir) if output_dir is not None else Paths.PREPROCESSED_DIR
            self.cleaning_level = cleaning_level
            self.min_chapter_length = min_chapter_length
            self.min_ratio = min_ratio
        
        self.export_global_jsonl = export_global_jsonl
        self.global_jsonl_file = Paths.ALL_NOVELS_PREPROCESSED_JSONL if export_global_jsonl else None
        self.global_paragraph_counter = 0
        
        # T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a t·ªìn t·∫°i
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Kh·ªüi t·∫°o dictionary ƒë·ªÉ l∆∞u th·ªëng k√™
        self.stats = {
            'total_chapters': 0,
            'processed_chapters': 0,
            'filtered_chapters': 0,
            'total_paragraphs': 0,
            'total_chars': 0,
            'total_bytes': 0,
            'novels': {},
            'filter_reasons': []
        }
    
    # ========================================================================
    # C√ÅC H√ÄM TI·ªÜN √çCH (UTILITY FUNCTIONS)
    # ========================================================================
    
    def extract_chapter_number(self, filename: str) -> int:
        """
        Tr√≠ch xu·∫•t s·ªë th·ª© t·ª± chapter t·ª´ t√™n file.
        
        Args:
            filename: T√™n file (v√≠ d·ª•: "chapter_123.txt")
        
        Returns:
            S·ªë th·ª© t·ª± chapter (v√≠ d·ª•: 123), ho·∫∑c 0 n·∫øu kh√¥ng t√¨m th·∫•y
        """
        match = CHAPTER_NUMBER_PATTERN.search(filename)
        if match:
            return int(match.group(1))
        return 0
    
    def is_dialogue_line(self, line: str, allow_longer: bool = False) -> bool:
        """
        Ki·ªÉm tra xem d√≤ng c√≥ ph·∫£i l√† h·ªôi tho·∫°i ng·∫Øn kh√¥ng.
        
        H·ªôi tho·∫°i ng·∫Øn th∆∞·ªùng c√≥:
        - D·∫•u ngo·∫∑c k√©p ·ªü ƒë·∫ßu/cu·ªëi
        - D·∫•u c√¢u ·ªü cu·ªëi (. ! ? ...)
        - ƒê·ªô d√†i ng·∫Øn nh∆∞ng h·ª£p l·ªá
        
        Args:
            line: D√≤ng text c·∫ßn ki·ªÉm tra
            allow_longer: True n·∫øu mu·ªën cho ph√©p ƒëo·∫°n d√†i h∆°n (<= MIN_PARAGRAPH_LENGTH)
        
        Returns:
            True n·∫øu l√† h·ªôi tho·∫°i ng·∫Øn h·ª£p l·ªá
        """
        stripped = line.strip()
        if not stripped:
            return False
        
        if allow_longer and len(stripped) <= MIN_PARAGRAPH_LENGTH:
            return bool(DIALOGUE_PATTERN.match(stripped))
        
        if len(stripped) <= MIN_LINE_LENGTH:
            return bool(DIALOGUE_PATTERN.match(stripped))
        return False
    
    # ========================================================================
    # C√ÅC H√ÄM L√ÄM S·∫†CH TEXT (TEXT CLEANING FUNCTIONS)
    # ========================================================================
    
    def remove_html_tags(self, text: str) -> str:
        """
        Lo·∫°i b·ªè HTML/XML tags v√† comments, nh∆∞ng gi·ªØ l·∫°i c·∫•u tr√∫c paragraph.
        
        QUAN TR·ªåNG:
            - Chuy·ªÉn <br> v√† <br/> th√†nh \n TR∆Ø·ªöC khi x√≥a tags
            - ƒêi·ªÅu n√†y gi·ªØ l·∫°i c·∫•u tr√∫c paragraph t·ª´ HTML
        
        M·ª©c ƒë·ªô: AGGRESSIVE (nh∆∞ng gi·ªØ c·∫•u tr√∫c)
        
        Args:
            text: Text c√≥ th·ªÉ ch·ª©a HTML tags
        
        Returns:
            Text ƒë√£ lo·∫°i b·ªè HTML tags nh∆∞ng gi·ªØ c·∫•u tr√∫c paragraph
        """
        # B∆∞·ªõc 1: X√≥a HTML comments tr∆∞·ªõc
        text = HTML_COMMENT_PATTERN.sub('', text)
        
        # B∆∞·ªõc 2: Chuy·ªÉn <br> v√† <br/> th√†nh newline (GI·ªÆ C·∫§U TR√öC)
        # ƒêi·ªÅu n√†y quan tr·ªçng v√¨ nhi·ªÅu site d√πng <br> ƒë·ªÉ xu·ªëng d√≤ng
        text = BR_TAG_PATTERN.sub('\n', text)
        
        # B∆∞·ªõc 3: X√≥a t·∫•t c·∫£ HTML tags c√≤n l·∫°i
        text = HTML_TAG_PATTERN.sub('', text)
        
        return text
    
    def remove_control_characters(self, text: str) -> str:
        """
        Lo·∫°i b·ªè control characters (k√Ω t·ª± ƒëi·ªÅu khi·ªÉn kh√¥ng in ƒë∆∞·ª£c).
        
        M·ª©c ƒë·ªô: SAFE
        - Gi·ªØ l·∫°i: \n (newline), \t (tab), space
        - X√≥a: T·∫•t c·∫£ control characters kh√°c
        
        Args:
            text: Text c√≥ th·ªÉ ch·ª©a control characters
        
        Returns:
            Text ƒë√£ lo·∫°i b·ªè control characters
        """
        return CONTROL_CHARS_PATTERN.sub('', text)
    
    def normalize_whitespace(self, text: str) -> str:
        """
        Chu·∫©n h√≥a whitespace (nhi·ªÅu spaces/tabs ‚Üí 1 space).
        
        M·ª©c ƒë·ªô: SAFE
        - Nhi·ªÅu spaces li√™n ti·∫øp ‚Üí 1 space
        - Nhi·ªÅu tabs li√™n ti·∫øp ‚Üí 1 space
        - Space + tab ‚Üí 1 space
        - L∆ØU √ù: Ch·ªâ normalize trong d√≤ng, kh√¥ng normalize line breaks
        
        Args:
            text: Text c√≥ th·ªÉ ch·ª©a nhi·ªÅu whitespace
        
        Returns:
            Text ƒë√£ chu·∫©n h√≥a whitespace
        """
        # Ch·ªâ normalize spaces/tabs trong d√≤ng, kh√¥ng normalize line breaks
        lines = text.split('\n')
        normalized_lines = [WHITESPACE_PATTERN.sub(' ', line) for line in lines]
        return '\n'.join(normalized_lines)
    
    def normalize_line_breaks(self, text: str) -> str:
        """
        Chu·∫©n h√≥a line breaks (xu·ªëng d√≤ng).
        
        M·ª©c ƒë·ªô: SAFE
        - \r\n (Windows) ‚Üí \n
        - \r (Mac old) ‚Üí \n
        - Nhi·ªÅu \n li√™n ti·∫øp (3+) ‚Üí 2 \n (gi·ªØ paragraph break)
        
        Args:
            text: Text c√≥ th·ªÉ ch·ª©a nhi·ªÅu lo·∫°i line breaks
        
        Returns:
            Text ƒë√£ chu·∫©n h√≥a line breaks
        """
        # Normalize Windows line breaks (\r\n ‚Üí \n)
        text = text.replace('\r\n', '\n')
        
        # Normalize Mac old line breaks (\r ‚Üí \n)
        text = text.replace('\r', '\n')
        
        # Nhi·ªÅu newlines li√™n ti·∫øp (3+) ‚Üí t·ªëi ƒëa 2 (gi·ªØ paragraph break)
        text = MULTIPLE_NEWLINES_PATTERN.sub('\n\n', text)
        
        return text
    
    def remove_special_characters(self, text: str) -> str:
        """
        Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát kh√¥ng c·∫ßn thi·∫øt D·ª∞A TR√äN UNICODE CATEGORIES.
        
        C·∫¢I THI·ªÜN: D√πng Unicode general categories thay v√¨ regex th·ªß c√¥ng.
        - Gi·ªØ l·∫°i: Letters (L), Numbers (N), Punctuation (P), Symbols (S) h·ª£p l·ªá
        - X√≥a: Control (C), Format (Cf), Private Use (Co), Surrogate (Cs)
        
        M·ª©c ƒë·ªô: MODERATE (BALANCED) ho·∫∑c AGGRESSIVE (t√πy cleaning_level)
        
        Args:
            text: Text c·∫ßn l√†m s·∫°ch
        
        Returns:
            Text ƒë√£ lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát (t√πy cleaning_level)
        """
        if self.cleaning_level == CleaningLevel.SAFE:
            # SAFE: Kh√¥ng x√≥a g√¨ c·∫£, ch·ªâ gi·ªØ l·∫°i
            return text
        
        result = []
        for char in text:
            # L·∫•y Unicode category
            category = unicodedata.category(char)
            char_code = ord(char)
            
            # Gi·ªØ l·∫°i:
            # - Letters (L): T·∫•t c·∫£ ch·ªØ c√°i (bao g·ªìm ti·∫øng Vi·ªát)
            # - Numbers (N): T·∫•t c·∫£ s·ªë
            # - Punctuation (P): D·∫•u c√¢u
            # - Symbols (S): M·ªôt s·ªë symbols h·ª£p l·ªá
            # - Whitespace: Space, \n, \t
            if category.startswith('L'):  # Letters
                result.append(char)
            elif category.startswith('N'):  # Numbers
                result.append(char)
            elif category.startswith('P'):  # Punctuation
                result.append(char)
            elif category.startswith('S'):  # Symbols
                # Ch·ªâ gi·ªØ l·∫°i m·ªôt s·ªë symbols h·ª£p l·ªá (kh√¥ng ph·∫£i emoji)
                # Emoji th∆∞·ªùng l√† So (Symbol, other) v·ªõi code point > 0x1F000
                if self.cleaning_level == CleaningLevel.BALANCED:
                    # BALANCED: Gi·ªØ emoji v√† symbols (code point < 0x1F000 l√† symbols th√¥ng th∆∞·ªùng)
                    if char_code < 0x1F000:
                        result.append(char)
                    else:
                        # Emoji (code point >= 0x1F000) - gi·ªØ l·∫°i trong BALANCED
                        result.append(char)
                else:
                    # AGGRESSIVE: Ch·ªâ gi·ªØ symbols th√¥ng th∆∞·ªùng, x√≥a emoji
                    if char_code < 0x1F000:
                        result.append(char)
                    # else: x√≥a emoji
            elif char in [' ', '\n', '\t']:  # Whitespace
                result.append(char)
            # else: X√≥a (Control, Format, Private Use, Surrogate, etc.)
        
        return ''.join(result)
    
    def trim_whitespace(self, text: str) -> str:
        """
        X√≥a whitespace ·ªü ƒë·∫ßu/cu·ªëi m·ªói d√≤ng v√† to√†n b·ªô text.
        
        M·ª©c ƒë·ªô: SAFE
        - X√≥a space ·ªü ƒë·∫ßu/cu·ªëi m·ªói d√≤ng
        - X√≥a space ·ªü ƒë·∫ßu/cu·ªëi to√†n b·ªô text
        - Gi·ªØ l·∫°i line breaks (\n)
        
        Args:
            text: Text c√≥ th·ªÉ c√≥ whitespace th·ª´a
        
        Returns:
            Text ƒë√£ trim whitespace
        """
        # Trim t·ª´ng d√≤ng
        lines = [line.strip() for line in text.split('\n')]
        text = '\n'.join(lines)
        
        # Trim to√†n b·ªô text
        text = text.strip()
        
        return text
    
    def remove_short_lines(self, text: str, min_length: int = MIN_LINE_LENGTH) -> str:
        """
        X√≥a c√°c d√≤ng qu√° ng·∫Øn, NH∆ØNG GI·ªÆ L·∫†I H·ªòI THO·∫†I NG·∫ÆN.
        
        C·∫¢I THI·ªÜN: Kh√¥ng x√≥a h·ªôi tho·∫°i ng·∫Øn h·ª£p l·ªá nh∆∞ "ƒê∆∞·ª£c.", "Kh√¥ng.", "A!"
        
        M·ª©c ƒë·ªô: MODERATE
        - D√≤ng c√≥ < min_length k√Ω t·ª± ‚Üí X√≥a (TR·ª™ h·ªôi tho·∫°i)
        - TR·ª™: D√≤ng ch·ªâ c√≥ s·ªë (c√≥ th·ªÉ l√† s·ªë ch∆∞∆°ng) ‚Üí Gi·ªØ l·∫°i
        - TR·ª™: D√≤ng l√† h·ªôi tho·∫°i ng·∫Øn ‚Üí Gi·ªØ l·∫°i
        
        Args:
            text: Text c√≥ th·ªÉ ch·ª©a d√≤ng ng·∫Øn
            min_length: ƒê·ªô d√†i t·ªëi thi·ªÉu c·ªßa d√≤ng (m·∫∑c ƒë·ªãnh: 10)
        
        Returns:
            Text ƒë√£ lo·∫°i b·ªè d√≤ng ng·∫Øn (nh∆∞ng gi·ªØ h·ªôi tho·∫°i)
        """
        lines = text.split('\n')
        filtered_lines = []
        
        for line in lines:
            stripped = line.strip()
            
            # Gi·ªØ l·∫°i n·∫øu ƒë·ªß d√†i
            if len(stripped) >= min_length:
                filtered_lines.append(line)
            # Gi·ªØ l·∫°i n·∫øu l√† s·ªë (c√≥ th·ªÉ l√† s·ªë ch∆∞∆°ng)
            elif stripped.isdigit():
                filtered_lines.append(line)
            # Gi·ªØ l·∫°i n·∫øu l√† h·ªôi tho·∫°i ng·∫Øn (C·∫¢I THI·ªÜN)
            elif self.is_dialogue_line(stripped):
                filtered_lines.append(line)
            # else: b·ªè qua d√≤ng ng·∫Øn
        
        return '\n'.join(filtered_lines)
    
    def clean_text(self, text: str) -> str:
        """
        H√†m t·ªïng h·ª£p: L√†m s·∫°ch text theo chi·∫øn l∆∞·ª£c (SAFE/BALANCED/AGGRESSIVE).
        
        QUAN TR·ªåNG: Th·ª© t·ª± x·ª≠ l√Ω ƒë∆∞·ª£c s·∫Øp x·∫øp l·∫°i ƒë·ªÉ tr√°nh conflict:
            1. Lo·∫°i b·ªè HTML tags (chuy·ªÉn <br> th√†nh \n tr∆∞·ªõc)
            2. Lo·∫°i b·ªè control characters
            3. Chu·∫©n h√≥a line breaks (TR∆Ø·ªöC normalize whitespace)
            4. Chu·∫©n h√≥a whitespace (SAU normalize line breaks)
            5. Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát (t√πy cleaning_level)
            6. Trim whitespace
            7. X√≥a d√≤ng qu√° ng·∫Øn (nh∆∞ng gi·ªØ h·ªôi tho·∫°i)
        
        Args:
            text: Text raw c·∫ßn l√†m s·∫°ch
        
        Returns:
            Text ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch
        """
        # B∆∞·ªõc 1: Lo·∫°i b·ªè HTML tags (chuy·ªÉn <br> th√†nh \n tr∆∞·ªõc) - AGGRESSIVE
        if self.cleaning_level != CleaningLevel.SAFE:
            text = self.remove_html_tags(text)
        
        # B∆∞·ªõc 2: Lo·∫°i b·ªè control characters - SAFE
        text = self.remove_control_characters(text)
        
        # B∆∞·ªõc 3: Chu·∫©n h√≥a line breaks TR∆Ø·ªöC normalize whitespace - SAFE
        # Quan tr·ªçng: Ph·∫£i normalize line breaks tr∆∞·ªõc ƒë·ªÉ gi·ªØ c·∫•u tr√∫c paragraph
        text = self.normalize_line_breaks(text)
        
        # B∆∞·ªõc 4: Chu·∫©n h√≥a whitespace SAU normalize line breaks - SAFE
        text = self.normalize_whitespace(text)
        
        # B∆∞·ªõc 5: Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát (t√πy cleaning_level) - MODERATE/AGGRESSIVE
        if self.cleaning_level != CleaningLevel.SAFE:
            text = self.remove_special_characters(text)
        
        # B∆∞·ªõc 6: Trim whitespace - SAFE
        text = self.trim_whitespace(text)
        
        # B∆∞·ªõc 7: X√≥a d√≤ng qu√° ng·∫Øn (nh∆∞ng gi·ªØ h·ªôi tho·∫°i) - MODERATE
        if self.cleaning_level != CleaningLevel.SAFE:
            text = self.remove_short_lines(text, min_length=MIN_LINE_LENGTH)
        
        return text
    
    # ========================================================================
    # C√ÅC H√ÄM CHIA ƒêO·∫†N VƒÇN (PARAGRAPH SEGMENTATION)
    # ========================================================================
    
    def split_into_paragraphs(self, text: str) -> List[str]:
        """
        Chia text th√†nh c√°c paragraphs (ƒëo·∫°n vƒÉn).
        
        Quy t·∫Øc:
            - T√°ch theo pattern: \n\s*\n (2 newlines li√™n ti·∫øp)
            - M·ªói paragraph l√† m·ªôt ƒëo·∫°n vƒÉn ƒë·ªôc l·∫≠p
        
        Args:
            text: Text c·∫ßn chia th√†nh paragraphs
        
        Returns:
            List c√°c paragraphs (ƒë√£ strip)
        """
        # Chia theo paragraph break (2 newlines li√™n ti·∫øp)
        paragraphs = PARAGRAPH_BREAK_PATTERN.split(text)
        
        # Strip v√† lo·∫°i b·ªè paragraph r·ªóng
        paragraphs = [p.strip() for p in paragraphs if p.strip()]
        
        return paragraphs
    
    def split_long_paragraph(self, paragraph: str, max_length: int = MAX_PARAGRAPH_LENGTH) -> List[str]:
        """
        Chia paragraph d√†i th√†nh nhi·ªÅu chunks nh·ªè h∆°n.
        
        C·∫¢I THI·ªÜN: C√≥ fallback theo ƒë·ªô d√†i n·∫øu kh√¥ng t√¨m th·∫•y d·∫•u c√¢u.
        
        Quy t·∫Øc:
            - N·∫øu paragraph <= max_length ‚Üí gi·ªØ nguy√™n
            - N·∫øu paragraph > max_length ‚Üí chia th√†nh chunks
            - ∆Øu ti√™n: Chia theo c√¢u (d·∫•u ch·∫•m, ch·∫•m h·ªèi, ch·∫•m than)
            - Fallback: Chia theo ƒë·ªô d√†i c·ªë ƒë·ªãnh n·∫øu kh√¥ng c√≥ d·∫•u c√¢u
        
        Args:
            paragraph: Paragraph c·∫ßn chia (n·∫øu qu√° d√†i)
            max_length: ƒê·ªô d√†i t·ªëi ƒëa c·ªßa m·ªói chunk (m·∫∑c ƒë·ªãnh: 2000)
        
        Returns:
            List c√°c chunks (c√≥ th·ªÉ ch·ªâ c√≥ 1 ph·∫ßn t·ª≠ n·∫øu kh√¥ng c·∫ßn chia)
        """
        # N·∫øu kh√¥ng qu√° d√†i th√¨ gi·ªØ nguy√™n
        if len(paragraph) <= max_length:
            return [paragraph]
        
        chunks = []
        current_chunk = ""
        
        # Th·ª≠ chia theo c√¢u (d·∫•u ch·∫•m, ch·∫•m h·ªèi, ch·∫•m than)
        sentences = SENTENCE_END_PATTERN.split(paragraph)
        
        # N·∫øu kh√¥ng t√¨m th·∫•y d·∫•u c√¢u (ch·ªâ c√≥ 1 ph·∫ßn t·ª≠), d√πng fallback
        if len(sentences) <= 1:
            # Fallback: Chia theo ƒë·ªô d√†i c·ªë ƒë·ªãnh
            for i in range(0, len(paragraph), max_length):
                chunk = paragraph[i:i + max_length]
                # T√¨m v·ªã tr√≠ space g·∫ßn nh·∫•t ƒë·ªÉ kh√¥ng c·∫Øt gi·ªØa t·ª´
                if i + max_length < len(paragraph):
                    last_space = chunk.rfind(' ')
                    if last_space > max_length * 0.8:  # N·∫øu space kh√¥ng qu√° xa
                        chunk = chunk[:last_space]
                chunks.append(chunk.strip())
            return [c for c in chunks if c]  # Lo·∫°i b·ªè chunk r·ªóng
        
        # Gh√©p l·∫°i sentences th√†nh chunks
        for i in range(0, len(sentences), 2):
            # L·∫•y sentence v√† d·∫•u c√¢u ƒëi k√®m
            sentence = sentences[i] + (sentences[i+1] if i+1 < len(sentences) else '')
            
            # N·∫øu th√™m sentence n√†y v·∫´n <= max_length th√¨ th√™m v√†o chunk hi·ªán t·∫°i
            if len(current_chunk) + len(sentence) <= max_length:
                current_chunk += sentence
            else:
                # N·∫øu chunk hi·ªán t·∫°i ƒë√£ c√≥ n·ªôi dung th√¨ l∆∞u l·∫°i
                if current_chunk:
                    chunks.append(current_chunk.strip())
                # B·∫Øt ƒë·∫ßu chunk m·ªõi
                current_chunk = sentence
        
        # L∆∞u chunk cu·ªëi c√πng
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def filter_valid_paragraphs(self, paragraphs: List[str]) -> List[str]:
        """
        L·ªçc c√°c paragraphs h·ª£p l·ªá (ƒë·ªô d√†i 50-2000 k√Ω t·ª±).
        
        Quy t·∫Øc:
            - Qu√° ng·∫Øn (< 50 k√Ω t·ª±): B·ªè qua (TR·ª™ h·ªôi tho·∫°i ng·∫Øn)
            - H·ª£p l·ªá (50-2000 k√Ω t·ª±): Gi·ªØ l·∫°i
            - Qu√° d√†i (> 2000 k√Ω t·ª±): Chia nh·ªè
        
        Args:
            paragraphs: List c√°c paragraphs c·∫ßn filter
        
        Returns:
            List c√°c paragraphs h·ª£p l·ªá (ƒë√£ chia nh·ªè n·∫øu c·∫ßn)
        """
        valid_paragraphs = []
        
        for para in paragraphs:
            length = len(para)
            
            # Qu√° ng·∫Øn ‚Üí b·ªè qua (TR·ª™ h·ªôi tho·∫°i ng·∫Øn)
            if length < MIN_PARAGRAPH_LENGTH:
                # Gi·ªØ h·ªôi tho·∫°i (cho ph√©p d√†i t·ªõi MIN_PARAGRAPH_LENGTH)
                if self.is_dialogue_line(para, allow_longer=True):
                    valid_paragraphs.append(para)
                    continue
                
                # Cho ph√©p gi·ªØ ƒëo·∫°n 30-49 k√Ω t·ª± n·∫øu c√≥ d·∫•u c√¢u k·∫øt th√∫c
                if length >= RELAXED_PARAGRAPH_MIN_LENGTH:
                    if any(punct in para for punct in '.!?‚Ä¶„ÄÇÔºÅÔºü'):
                        valid_paragraphs.append(para)
                        continue
                # else: b·ªè qua ƒëo·∫°n qu√° ng·∫Øn
                continue
            
            # H·ª£p l·ªá ‚Üí gi·ªØ l·∫°i
            elif length <= MAX_PARAGRAPH_LENGTH:
                valid_paragraphs.append(para)
            
            # Qu√° d√†i ‚Üí chia nh·ªè
            else:
                chunks = self.split_long_paragraph(para, max_length=MAX_PARAGRAPH_LENGTH)
                valid_paragraphs.extend(chunks)
        
        return valid_paragraphs
    
    # ========================================================================
    # C√ÅC H√ÄM FILTER CHAPTER
    # ========================================================================
    
    def should_filter_chapter(
        self,
        content: str,
        paragraphs: List[str],
        novel_avg_size: Optional[float] = None
    ) -> Tuple[bool, str]:
        """
        Quy·∫øt ƒë·ªãnh c√≥ n√™n filter chapter n√†y kh√¥ng.
        
        C·∫¢I THI·ªÜN: N·∫øu chapter c√≥ > 1 paragraph h·ª£p l·ªá ‚Üí KH√îNG FILTER
        (Tr√°nh filter oan chapter ng·∫Øn nh∆∞ng c√≥ nhi·ªÅu paragraphs h·ª£p l·ªá)
        
        Quy t·∫Øc:
            1. N·∫øu c√≥ > 1 paragraph h·ª£p l·ªá ‚Üí KH√îNG FILTER
            2. ƒê·ªô d√†i < min_chapter_length (bytes) ‚Üí Filter
            3. ƒê·ªô d√†i < min_ratio * trung b√¨nh ‚Üí Filter
        
        Args:
            content: N·ªôi dung chapter (ƒë√£ l√†m s·∫°ch)
            paragraphs: List c√°c paragraphs h·ª£p l·ªá
            novel_avg_size: ƒê·ªô d√†i trung b√¨nh c·ªßa truy·ªán (bytes), None n·∫øu ch∆∞a t√≠nh
        
        Returns:
            Tuple (should_filter, reason):
                - should_filter: True n·∫øu n√™n filter, False n·∫øu gi·ªØ l·∫°i
                - reason: L√Ω do filter (n·∫øu c√≥)
        """
        # C·∫¢I THI·ªÜN: N·∫øu c√≥ > 1 paragraph h·ª£p l·ªá ‚Üí KH√îNG FILTER
        if len(paragraphs) > 1:
            return False, ""
        
        content_bytes = len(content.encode('utf-8'))
        
        # Check 1: ƒê·ªô d√†i t·ªëi thi·ªÉu tuy·ªát ƒë·ªëi
        if content_bytes < self.min_chapter_length:
            return True, f"ƒê·ªô d√†i < {self.min_chapter_length} bytes v√† ch·ªâ c√≥ {len(paragraphs)} paragraph"
        
        # Check 2: ƒê·ªô d√†i so v·ªõi trung b√¨nh (n·∫øu c√≥)
        if novel_avg_size and novel_avg_size > 0:
            min_size = novel_avg_size * self.min_ratio
            if content_bytes < min_size:
                return True, f"ƒê·ªô d√†i < {self.min_ratio*100}% trung b√¨nh ({min_size:.0f} bytes) v√† ch·ªâ c√≥ {len(paragraphs)} paragraph"
        
        return False, ""
    
    # ========================================================================
    # C√ÅC H√ÄM X·ª¨ L√ù NOVEL
    # ========================================================================
    
    def process_novel(self, novel_dir: Path) -> Optional[Dict]:
        """
        X·ª≠ l√Ω m·ªôt truy·ªán: ƒë·ªçc, l√†m s·∫°ch, filter, chia paragraphs.
        
        Quy tr√¨nh:
            1. T√¨m t·∫•t c·∫£ file chapter
            2. T√≠nh ƒë·ªô d√†i trung b√¨nh (ƒë·ªÉ filter)
            3. X·ª≠ l√Ω t·ª´ng chapter:
                - ƒê·ªçc file
                - L√†m s·∫°ch text
                - Chia th√†nh paragraphs
                - Filter paragraphs h·ª£p l·ªá
                - Check filter chapter (c√≥ exception cho nhi·ªÅu paragraphs)
            4. Thu th·∫≠p th·ªëng k√™
        
        Args:
            novel_dir: ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c ch·ª©a chapters c·ªßa truy·ªán
        
        Returns:
            Dict ch·ª©a stats v√† paragraphs, ho·∫∑c None n·∫øu l·ªói
        """
        novel_name = novel_dir.name
        print(f"\nüìñ X·ª≠ l√Ω: {novel_name}")
        
        # T√¨m t·∫•t c·∫£ file chapter (pattern: chapter_*.txt)
        chapter_files = list(novel_dir.glob("chapter_*.txt"))
        
        # S·∫Øp x·∫øp theo s·ªë th·ª© t·ª± chapter
        chapter_files.sort(key=lambda x: self.extract_chapter_number(x.name))
        
        if not chapter_files:
            print(f"  ‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y file chapter n√†o")
            return None
        
        print(f"  üìÑ T√¨m th·∫•y {len(chapter_files)} chapters")
        
        # T√≠nh ƒë·ªô d√†i trung b√¨nh c·ªßa chapters (ƒë·ªÉ filter)
        chapter_sizes = []
        for chapter_file in chapter_files:
            try:
                size = chapter_file.stat().st_size
                chapter_sizes.append(size)
            except Exception as e:
                print(f"  ‚ö†Ô∏è  Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c size c·ªßa {chapter_file.name}: {e}")
        
        avg_size = sum(chapter_sizes) / len(chapter_sizes) if chapter_sizes else 0
        print(f"  üìä ƒê·ªô d√†i trung b√¨nh: {avg_size / 1024:.2f} KB")
        
        # X·ª≠ l√Ω t·ª´ng chapter
        processed_paragraphs = []
        processed_chapters = 0
        filtered_chapters = 0
        
        for chapter_file in tqdm(chapter_files, desc=f"  ƒêang x·ª≠ l√Ω", leave=False):
            try:
                chapter_index = self.extract_chapter_number(chapter_file.name)
                
                # ƒê·ªçc file v·ªõi encoding UTF-8
                with open(chapter_file, 'r', encoding='utf-8') as f:
                    raw_content = f.read()
                
                # L√†m s·∫°ch text
                cleaned_content = self.clean_text(raw_content)
                
                # Chia th√†nh paragraphs
                paragraphs = self.split_into_paragraphs(cleaned_content)
                
                # Filter paragraphs h·ª£p l·ªá
                valid_paragraphs = self.filter_valid_paragraphs(paragraphs)
                
                # Check filter chapter (C·∫¢I THI·ªÜN: c√≥ exception cho nhi·ªÅu paragraphs)
                should_filter, reason = self.should_filter_chapter(cleaned_content, valid_paragraphs, avg_size)
                if should_filter:
                    filtered_chapters += 1
                    reason_text = reason or "Kh√¥ng r√µ l√Ω do"
                    print(f"  üóëÔ∏è  B·ªè {chapter_file.name}: {reason_text}")
                    self.stats['filter_reasons'].append({
                        'novel_name': novel_name,
                        'chapter_file': chapter_file.name,
                        'chapter_index': chapter_index,
                        'reason': reason_text
                    })
                    continue
                
                # Th√™m v√†o list
                processed_paragraphs.extend(valid_paragraphs)
                processed_chapters += 1
                
            except Exception as e:
                print(f"  ‚ùå L·ªói khi x·ª≠ l√Ω {chapter_file.name}: {e}")
                continue
        
        # Th·ªëng k√™
        total_chars = sum(len(p) for p in processed_paragraphs)
        total_bytes = sum(len(p.encode('utf-8')) for p in processed_paragraphs)
        
        novel_stats = {
            'novel_name': novel_name,
            'total_chapters': len(chapter_files),
            'processed_chapters': processed_chapters,
            'filtered_chapters': filtered_chapters,
            'total_paragraphs': len(processed_paragraphs),
            'total_chars': total_chars,
            'total_bytes': total_bytes,
            'avg_chars_per_chapter': total_chars / processed_chapters if processed_chapters > 0 else 0,
            'avg_chars_per_paragraph': total_chars / len(processed_paragraphs) if processed_paragraphs else 0
        }
        
        print(f"  ‚úÖ ƒê√£ x·ª≠ l√Ω: {processed_chapters}/{len(chapter_files)} chapters")
        print(f"  üóëÔ∏è  ƒê√£ filter: {filtered_chapters} chapters")
        print(f"  üìù T·ªïng paragraphs: {len(processed_paragraphs):,}")
        print(f"  üìä T·ªïng k√Ω t·ª±: {total_chars:,}")
        
        return {
            'stats': novel_stats,
            'paragraphs': processed_paragraphs
        }
    
    # ========================================================================
    # C√ÅC H√ÄM L∆ØU K·∫æT QU·∫¢
    # ========================================================================
    
    def save_preprocessed(
        self,
        novel_data: Dict,
        format: Literal['combined', 'jsonl'] = 'combined'
    ) -> None:
        """
        L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω v√†o file.
        
        Format:
            - 'combined': 1 file .txt l·ªõn cho m·ªói truy·ªán (khuy·∫øn ngh·ªã)
            - 'jsonl': JSONL format chu·∫©n LLM (m·ªói d√≤ng l√† 1 JSON object)
        
        Args:
            novel_data: Dict ch·ª©a stats v√† paragraphs
            format: Format output ('combined' ho·∫∑c 'jsonl')
        """
        if not novel_data:
            return
        
        novel_name = novel_data['stats']['novel_name']
        paragraphs = novel_data['paragraphs']
        
        if format == 'combined':
            # L∆∞u th√†nh 1 file .txt l·ªõn
            output_file = self.output_dir / f"{novel_name}_preprocessed.txt"
            
            with open(output_file, 'w', encoding='utf-8') as f:
                # Ghi t·ª´ng paragraph, c√°ch nhau b·ªüi 2 newlines
                for para in paragraphs:
                    f.write(para)
                    f.write('\n\n')  # Separator gi·ªØa c√°c paragraph
            
            print(f"  üíæ ƒê√£ l∆∞u: {output_file}")
        
        elif format == 'jsonl':
            # L∆∞u th√†nh JSONL format (chu·∫©n LLM)
            output_file = self.output_dir / f"{novel_name}_preprocessed.jsonl"
            
            with open(output_file, 'w', encoding='utf-8') as f:
                for i, para in enumerate(paragraphs):
                    # M·ªói d√≤ng l√† 1 JSON object
                    json_obj = {
                        "text": para,
                        "novel_name": novel_name,
                        "paragraph_index": i
                    }
                    f.write(json.dumps(json_obj, ensure_ascii=False) + '\n')
            
            print(f"  üíæ ƒê√£ l∆∞u: {output_file} (JSONL format)")
        
        # L∆∞u metadata
        metadata_file = self.output_dir / f"{novel_name}_metadata.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(novel_data['stats'], f, ensure_ascii=False, indent=2)

    def append_to_global_jsonl(self, novel_data: Dict) -> None:
        """
        B·ªï sung paragraphs c·ªßa m·ªôt truy·ªán v√†o file JSONL t·ªïng (n·∫øu b·∫≠t).
        
        Args:
            novel_data: Dict ch·ª©a th√¥ng tin truy·ªán sau preprocessing
        """
        if not (self.export_global_jsonl and novel_data):
            return
        
        paragraphs = novel_data['paragraphs']
        novel_name = novel_data['stats']['novel_name']
        
        with open(self.global_jsonl_file, 'a', encoding='utf-8') as f:
            for idx, para in enumerate(paragraphs):
                json_obj = {
                    "text": para,
                    "novel_name": novel_name,
                    "paragraph_index": idx,
                    "global_paragraph_index": self.global_paragraph_counter
                }
                f.write(json.dumps(json_obj, ensure_ascii=False) + '\n')
                self.global_paragraph_counter += 1
    
    def save_summary(self) -> None:
        """L∆∞u file th·ªëng k√™ t·ªïng c·ªßa to√†n b·ªô qu√° tr√¨nh preprocessing."""
        summary_file = Paths.PREPROCESSING_SUMMARY_JSON
        
        summary = {
            'preprocessing_config': {
                'cleaning_level': self.cleaning_level.value,
                'min_chapter_length': self.min_chapter_length,
                'min_ratio': self.min_ratio,
                'min_paragraph_length': MIN_PARAGRAPH_LENGTH,
                'relaxed_paragraph_length': RELAXED_PARAGRAPH_MIN_LENGTH,
                'max_paragraph_length': MAX_PARAGRAPH_LENGTH,
                'export_global_jsonl': self.export_global_jsonl,
                'global_jsonl_file': str(self.global_jsonl_file) if self.export_global_jsonl else None
            },
            'statistics': {
                'total_novels': len(self.stats['novels']),
                'total_chapters': self.stats['total_chapters'],
                'processed_chapters': self.stats['processed_chapters'],
                'filtered_chapters': self.stats['filtered_chapters'],
                'total_paragraphs': self.stats['total_paragraphs'],
                'total_chars': self.stats['total_chars'],
                'total_bytes': self.stats['total_bytes'],
                'global_paragraphs': self.global_paragraph_counter if self.export_global_jsonl else 0
            },
            'novels': self.stats['novels'],
            'filter_reasons': self.stats['filter_reasons']
        }
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"\nüíæ ƒê√£ l∆∞u th·ªëng k√™: {summary_file}")
    
    def print_summary(self) -> None:
        """In th·ªëng k√™ t·ªïng ra console."""
        print("\n" + "=" * 80)
        print("üìä TH·ªêNG K√ä T·ªîNG")
        print("=" * 80)
        print(f"üìö T·ªïng s·ªë truy·ªán: {len(self.stats['novels'])}")
        print(f"üìÑ T·ªïng s·ªë chapters: {self.stats['total_chapters']:,}")
        print(f"‚úÖ ƒê√£ x·ª≠ l√Ω: {self.stats['processed_chapters']:,}")
        print(f"üóëÔ∏è  ƒê√£ filter: {self.stats['filtered_chapters']:,}")
        print(f"üìù T·ªïng paragraphs: {self.stats['total_paragraphs']:,}")
        print(f"üìä T·ªïng k√Ω t·ª±: {self.stats['total_chars']:,}")
        print(f"üíæ T·ªïng dung l∆∞·ª£ng: {self.stats['total_bytes'] / (1024*1024):.2f} MB")
        if self.stats['total_paragraphs'] > 0:
            print(f"üìà Trung b√¨nh: {self.stats['total_chars'] / self.stats['total_paragraphs']:.0f} k√Ω t·ª±/paragraph")
        if self.export_global_jsonl:
            print(f"üßæ Global JSONL: {self.global_paragraph_counter:,} paragraphs ‚Üí {self.global_jsonl_file}")
        print("=" * 80)
    
    # ========================================================================
    # H√ÄM CH√çNH - CH·∫†Y PREPROCESSING
    # ========================================================================
    
    def run(self, format: Literal['combined', 'jsonl'] = 'combined') -> None:
        """
        H√†m ch√≠nh: Ch·∫°y preprocessing cho t·∫•t c·∫£ truy·ªán.
        
        Args:
            format: Format output ('combined' ho·∫∑c 'jsonl')
        """
        print("=" * 80)
        print("üöÄ B·∫ÆT ƒê·∫¶U PREPROCESSING")
        print("=" * 80)
        print(f"üìÅ Input: {self.raw_dir}")
        print(f"üìÅ Output: {self.output_dir}")
        print(f"‚öôÔ∏è  Cleaning Level: {self.cleaning_level.value.upper()}")
        print(f"‚öôÔ∏è  Format: {format}")
        print(f"üîß Min chapter length: {self.min_chapter_length} bytes")
        print(f"üîß Min ratio: {self.min_ratio * 100}% trung b√¨nh")
        if self.export_global_jsonl:
            print(f"üßæ Global JSONL: {self.global_jsonl_file}")
        print("=" * 80)
        
        # Chu·∫©n b·ªã file global JSONL (n·∫øu b·∫≠t)
        if self.export_global_jsonl:
            try:
                if self.global_jsonl_file.exists():
                    self.global_jsonl_file.unlink()
            except Exception as exc:
                print(f"‚ö†Ô∏è  Kh√¥ng x√≥a ƒë∆∞·ª£c file c≈© {self.global_jsonl_file}: {exc}")
            self.global_paragraph_counter = 0
        
        # T√¨m t·∫•t c·∫£ folder truy·ªán
        novel_dirs = [d for d in self.raw_dir.iterdir() if d.is_dir()]
        
        if not novel_dirs:
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y folder truy·ªán n√†o trong {self.raw_dir}")
            return
        
        print(f"\nüìö T√¨m th·∫•y {len(novel_dirs)} truy·ªán\n")
        
        # X·ª≠ l√Ω t·ª´ng truy·ªán
        for novel_dir in novel_dirs:
            novel_data = self.process_novel(novel_dir)
            
            if novel_data:
                # L∆∞u d·ªØ li·ªáu
                self.save_preprocessed(novel_data, format=format)
                # Ghi th√™m v√†o file JSONL t·ªïng n·∫øu c·∫ßn
                self.append_to_global_jsonl(novel_data)
                
                # C·∫≠p nh·∫≠t th·ªëng k√™
                stats = novel_data['stats']
                self.stats['total_chapters'] += stats['total_chapters']
                self.stats['processed_chapters'] += stats['processed_chapters']
                self.stats['filtered_chapters'] += stats['filtered_chapters']
                self.stats['total_paragraphs'] += stats['total_paragraphs']
                self.stats['total_chars'] += stats['total_chars']
                self.stats['total_bytes'] += stats['total_bytes']
                self.stats['novels'][stats['novel_name']] = stats
        
        # L∆∞u th·ªëng k√™ t·ªïng
        self.save_summary()
        
        # In k·∫øt qu·∫£
        self.print_summary()


# ============================================================================
# H√ÄM MAIN - ENTRY POINT
# ============================================================================

def main():
    """
    H√†m main: Parse arguments v√† ch·∫°y preprocessing.
    
    Arguments:
        --raw-dir: Th∆∞ m·ª•c ch·ª©a d·ªØ li·ªáu raw
        --output-dir: Th∆∞ m·ª•c output
        --cleaning-level: M·ª©c ƒë·ªô l√†m s·∫°ch (safe/balanced/aggressive)
        --format: Format output (combined/jsonl)
        --min-length: ƒê·ªô d√†i t·ªëi thi·ªÉu chapter (bytes)
        --min-ratio: T·ª∑ l·ªá t·ªëi thi·ªÉu so v·ªõi trung b√¨nh
    """
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Preprocessing d·ªØ li·ªáu truy·ªán ti·∫øng Vi·ªát',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
V√≠ d·ª•:
  # Ch·∫°y v·ªõi default settings (BALANCED)
  python preprocessing.py
  
  # Ch·ªçn cleaning level
  python preprocessing.py --cleaning-level safe
  python preprocessing.py --cleaning-level balanced
  python preprocessing.py --cleaning-level aggressive
  
  # Xu·∫•t JSONL format
  python preprocessing.py --format jsonl
  
  # T√πy ch·ªânh filter
  python preprocessing.py --min-length 500 --min-ratio 0.1
        """
    )
    
    parser.add_argument(
        '--raw-dir',
        type=str,
        default='training/dataset/raw/truyenmoiii_output',
        help='Th∆∞ m·ª•c ch·ª©a d·ªØ li·ªáu raw'
    )
    
    parser.add_argument(
        '--output-dir',
        type=str,
        default='training/dataset/preprocessed',
        help='Th∆∞ m·ª•c output sau preprocessing'
    )
    
    parser.add_argument(
        '--cleaning-level',
        type=str,
        choices=['safe', 'balanced', 'aggressive'],
        default='balanced',
        help='M·ª©c ƒë·ªô l√†m s·∫°ch: safe (ch·ªâ normalize), balanced (gi·ªØ emoji), aggressive (x√≥a emoji)'
    )
    
    parser.add_argument(
        '--format',
        type=str,
        choices=['combined', 'jsonl'],
        default='combined',
        help='Format output: combined (.txt) ho·∫∑c jsonl (.jsonl)'
    )
    
    parser.add_argument(
        '--min-length',
        type=int,
        default=MIN_CHAPTER_LENGTH_BYTES,
        help=f'ƒê·ªô d√†i t·ªëi thi·ªÉu c·ªßa chapter (bytes) (m·∫∑c ƒë·ªãnh: {MIN_CHAPTER_LENGTH_BYTES})'
    )
    
    parser.add_argument(
        '--min-ratio',
        type=float,
        default=MIN_CHAPTER_RATIO,
        help=f'T·ª∑ l·ªá t·ªëi thi·ªÉu so v·ªõi trung b√¨nh (0.1 = 10%%) (m·∫∑c ƒë·ªãnh: {MIN_CHAPTER_RATIO})'
    )
    
    parser.add_argument(
        '--global-jsonl',
        action='store_true',
        help='Xu·∫•t th√™m file all_novels_preprocessed.jsonl (gom to√†n b·ªô ƒëo·∫°n vƒÉn)'
    )
    
    args = parser.parse_args()
    
    # Chuy·ªÉn ƒë·ªïi cleaning level string th√†nh Enum
    cleaning_level_map = {
        'safe': CleaningLevel.SAFE,
        'balanced': CleaningLevel.BALANCED,
        'aggressive': CleaningLevel.AGGRESSIVE
    }
    cleaning_level = cleaning_level_map[args.cleaning_level]
    
    # T·∫°o preprocessor
    preprocessor = Preprocessor(
        raw_dir=args.raw_dir,
        output_dir=args.output_dir,
        cleaning_level=cleaning_level,
        min_chapter_length=args.min_length,
        min_ratio=args.min_ratio,
        export_global_jsonl=args.global_jsonl
    )
    
    # Ch·∫°y preprocessing
    preprocessor.run(format=args.format)


if __name__ == "__main__":
    main()
