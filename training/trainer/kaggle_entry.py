"""
Kaggle runner: pack tokenized JSONL â†’ .pt rá»“i gá»i train_lm.run_training trong má»™t script.

Máº·c Ä‘á»‹nh ká»³ vá»ng báº¡n Ä‘Ã£ upload gÃ³i dá»¯ liá»‡u (táº¡o báº±ng prepare_kaggle_bundle.py) lÃªn Kaggle.
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Dict, List, Optional

import torch

try:
    import sentencepiece as spm  # type: ignore
except ImportError:  # pragma: no cover - optional
    spm = None

from .pack_tokenized_dataset import pack_sequences
from .train_lm import run_training
from .utils import ensure_dir, setup_encoding


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="One-click Kaggle training runner.")
    parser.add_argument(
        "--dataset-root",
        type=Path,
        default=Path("/kaggle/input/novel-lm-dataset"),
        help="ThÆ° má»¥c Kaggle dataset Ä‘Ã£ upload (bundle).",
    )
    parser.add_argument(
        "--config",
        type=Path,
        help="ÄÆ°á»ng dáº«n training_config.json (máº·c Ä‘á»‹nh: <dataset-root>/config/training_config.json).",
    )
    parser.add_argument(
        "--tokens-dir",
        type=Path,
        help="ThÆ° má»¥c chá»©a *_tokens.jsonl (máº·c Ä‘á»‹nh: <dataset-root>/dataset/tokenized).",
    )
    parser.add_argument(
        "--tokenizer-model",
        type=Path,
        help="ÄÆ°á»ng dáº«n SentencePiece .model Ä‘á»ƒ xÃ¡c Ä‘á»‹nh pad_id (máº·c Ä‘á»‹nh: <dataset-root>/tokenizer/sp_model.model).",
    )
    parser.add_argument(
        "--work-dir",
        type=Path,
        default=Path("/kaggle/working"),
        help="ThÆ° má»¥c lÃ m viá»‡c trÃªn Kaggle.",
    )
    parser.add_argument(
        "--pack-dir",
        type=Path,
        help="ThÆ° má»¥c lÆ°u .pt (máº·c Ä‘á»‹nh: <work-dir>/packed_seq_<seq_len>).",
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        help="ThÆ° má»¥c lÆ°u checkpoint (máº·c Ä‘á»‹nh: <work-dir>/model_output).",
    )
    parser.add_argument("--seq-len", type=int, default=1024)
    parser.add_argument("--stride", type=int, default=1024)
    parser.add_argument("--drop-remainder", action="store_true")
    parser.add_argument("--include-test", action="store_true", help="Pack thÃªm test split.")
    parser.add_argument("--skip-pack", action="store_true", help="Bá» qua bÆ°á»›c pack (dÃ¹ng sáºµn .pt).")
    parser.add_argument("--skip-train", action="store_true", help="Chá»‰ pack, khÃ´ng train.")
    parser.add_argument("--train-bin", type=Path, help="ÄÆ°á»ng dáº«n train .pt náº¿u skip-pack.")
    parser.add_argument("--val-bin", type=Path, help="ÄÆ°á»ng dáº«n val .pt náº¿u skip-pack.")
    parser.add_argument("--pad-token-id", type=int, help="Override pad token ID.")
    parser.add_argument("--resume", type=Path, help="Checkpoint Ä‘á»ƒ resume training.")
    parser.add_argument("--device", type=str, help="Thiáº¿t bá»‹ ('cuda', 'cpu', ...).")
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--show-progress", action="store_true")
    return parser.parse_args()


def load_training_config(config_path: Path) -> Dict:
    if not config_path.exists():
        raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y config: {config_path}")
    with open(config_path, "r", encoding="utf-8") as f:
        return json.load(f)


def resolve_pad_id(explicit: Optional[int], cfg: Dict, tokenizer_model: Optional[Path]) -> int:
    if explicit is not None:
        return explicit
    pad_from_cfg = cfg.get("training", {}).get("pad_token_id")
    if pad_from_cfg is not None:
        return pad_from_cfg
    model_path = tokenizer_model
    if model_path and model_path.exists() and spm is not None:
        sp = spm.SentencePieceProcessor(model_file=str(model_path))
        pad_id = sp.pad_id()
        if pad_id >= 0:
            return pad_id
        return sp.unk_id()
    print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y pad_token_id trong config. DÃ¹ng máº·c Ä‘á»‹nh 0.")
    return 0


def pack_if_needed(
    split_names: List[str],
    token_files: Dict[str, Path],
    pack_dir: Path,
    seq_len: int,
    stride: int,
    drop_remainder: bool,
    pad_token_id: int,
    show_progress: bool,
) -> Dict[str, Path]:
    ensure_dir(pack_dir)
    produced: Dict[str, Path] = {}
    for split in split_names:
        input_path = token_files[split]
        if not input_path.exists():
            raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y tokenized JSONL cho split '{split}': {input_path}")
        output_path = pack_dir / f"{split}_{seq_len}.pt"
        print(f"ğŸ” Packing {split}: {input_path} â†’ {output_path}")
        tensor, stats = pack_sequences(
            input_path=input_path,
            seq_len=seq_len,
            stride=stride,
            drop_remainder=drop_remainder,
            pad_token_id=pad_token_id,
            show_progress=show_progress,
        )
        meta = {
            "num_sequences": tensor.size(0),
            "seq_len": tensor.size(1),
            "split": split,
            "pad_token_id": pad_token_id,
            "stride": stride,
            "drop_remainder": drop_remainder,
            **stats,
        }
        torch.save({"input_ids": tensor, "meta": meta}, output_path)
        produced[split] = output_path
        print(
            f"âœ… {split}: {tensor.size(0):,} sequences | total_tokens={stats['total_input_tokens']:,} "
            f"| pad_seq={stats['padded_sequences']}"
        )
    return produced


def main() -> None:
    setup_encoding()
    args = parse_args()

    dataset_root = args.dataset_root
    config_path = args.config or (dataset_root / "config" / "training_config.json")
    cfg = load_training_config(config_path)

    tokens_dir = args.tokens_dir or (dataset_root / "dataset" / "tokenized")
    token_files = {
        "train": tokens_dir / "train_tokens.jsonl",
        "val": tokens_dir / "val_tokens.jsonl",
        "test": tokens_dir / "test_tokens.jsonl",
    }
    tokenizer_model = args.tokenizer_model or (dataset_root / "tokenizer" / "sp_model.model")

    pack_dir = args.pack_dir or (args.work_dir / f"packed_seq_{args.seq_len}")
    output_dir = args.output_dir or (args.work_dir / "model_output")

    pad_token_id = resolve_pad_id(args.pad_token_id, cfg, tokenizer_model)
    print(f"â„¹ï¸ pad_token_id = {pad_token_id}")

    split_names = ["train", "val"]
    if args.include_test:
        split_names.append("test")

    packed_paths: Dict[str, Path] = {}
    if not args.skip_pack:
        packed_paths = pack_if_needed(
            split_names=split_names,
            token_files=token_files,
            pack_dir=pack_dir,
            seq_len=args.seq_len,
            stride=args.stride,
            drop_remainder=args.drop_remainder,
            pad_token_id=pad_token_id,
            show_progress=args.show_progress,
        )
    else:
        print("âš ï¸ Skip pack enabled. Sáº½ dÃ¹ng Ä‘Æ°á»ng dáº«n .pt do báº¡n cung cáº¥p.")

    train_bin = args.train_bin or packed_paths.get("train")
    val_bin = args.val_bin or packed_paths.get("val")
    if not train_bin or not val_bin:
        raise ValueError("Thiáº¿u train_bin/val_bin. Cáº§n pack hoáº·c truyá»n --train-bin / --val-bin.")

    if args.skip_train:
        print("â­ï¸ ÄÃ£ skip training, chá»‰ thá»±c hiá»‡n pack.")
        return

    run_training(
        config_path=config_path,
        train_bin_override=train_bin,
        val_bin_override=val_bin,
        output_dir_override=output_dir,
        resume=args.resume,
        seed=args.seed,
        device_str=args.device,
    )


if __name__ == "__main__":
    main()

