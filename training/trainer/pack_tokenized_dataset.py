"""
Pack tokenized JSONL paragraphs thÃ nh fixed-length sequence tensors (.pt).

Sá»­ dá»¥ng khi chuáº©n bá»‹ dá»¯ liá»‡u cho training loop trÃªn Kaggle (hoáº·c mÃ´i trÆ°á»ng GPU).

VÃ­ dá»¥:
    python -m training.trainer.pack_tokenized_dataset \
        --input training/dataset/tokenized/train_tokens.jsonl \
        --output training/dataset/tokenized/train_1024.pt \
        --seq-len 1024 \
        --stride 1024 \
        --show-progress
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import torch
from tqdm import tqdm

try:
    import sentencepiece as spm  # type: ignore
except ImportError:  # pragma: no cover - optional dependency
    spm = None

from .utils import ensure_dir, setup_encoding


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Pack tokenized JSONL vÃ o fixed-length tensor (.pt)."
    )
    parser.add_argument(
        "--input",
        type=Path,
        required=True,
        help="ÄÆ°á»ng dáº«n JSONL chá»©a token IDs (tá»« tokenize_dataset.py).",
    )
    parser.add_argument(
        "--output",
        type=Path,
        required=True,
        help="ÄÆ°á»ng dáº«n file .pt sáº½ lÆ°u tensor packed sequences.",
    )
    parser.add_argument(
        "--seq-len",
        type=int,
        default=1024,
        help="Chiá»u dÃ i sequence cá»‘ Ä‘á»‹nh (máº·c Ä‘á»‹nh: 1024).",
    )
    parser.add_argument(
        "--stride",
        type=int,
        default=1024,
        help="Sá»‘ token dá»‹ch má»—i láº§n cáº¯t (máº·c Ä‘á»‹nh = seq_len, tá»©c lÃ  khÃ´ng overlap).",
    )
    parser.add_argument(
        "--drop-remainder",
        action="store_true",
        help="Bá» pháº§n dÆ° cuá»‘i cÃ¹ng náº¿u chÆ°a Ä‘á»§ seq_len (máº·c Ä‘á»‹nh: giá»¯ vÃ  padding).",
    )
    parser.add_argument(
        "--pad-token-id",
        type=int,
        default=None,
        help="Token ID dÃ¹ng Ä‘á»ƒ padding náº¿u giá»¯ remainder. Náº¿u khÃ´ng cung cáº¥p sáº½ "
        "cá»‘ gáº¯ng Ä‘á»c tá»« tokenizer SentencePiece (náº¿u chá»‰ Ä‘á»‹nh).",
    )
    parser.add_argument(
        "--tokenizer-model",
        type=Path,
        default=None,
        help="ÄÆ°á»ng dáº«n SentencePiece .model (náº¿u muá»‘n auto láº¥y pad_id).",
    )
    parser.add_argument(
        "--show-progress",
        action="store_true",
        help="Hiá»ƒn thá»‹ progress bar khi Ä‘á»c file JSONL.",
    )
    return parser.parse_args()


def pack_sequences(
    input_path: Path,
    seq_len: int,
    stride: int,
    drop_remainder: bool,
    pad_token_id: int,
    show_progress: bool,
) -> Tuple[torch.Tensor, Dict[str, int]]:
    """Äá»c JSONL token IDs vÃ  pack thÃ nh tensor cá»‘ Ä‘á»‹nh."""
    if stride <= 0:
        print(f"âš ï¸ stride={stride} khÃ´ng há»£p lá»‡. Auto set = seq_len ({seq_len}).")
        stride = seq_len
    if stride > seq_len:
        stride = seq_len
    elif stride < seq_len:
        print(
            f"âš ï¸ stride ({stride}) < seq_len ({seq_len}). "
            "Báº¡n Ä‘ang báº­t cháº¿ Ä‘á»™ sliding window (overlap)."
        )

    stats = {
        "total_input_tokens": 0,
        "total_output_sequences": 0,
        "invalid_records": 0,
        "empty_records": 0,
        "padded_sequences": 0,
        "dropped_tokens": 0,
    }

    sequences: List[List[int]] = []
    buffer: List[int] = []

    open_kwargs = {"encoding": "utf-8"}

    def iter_lines():
        with open(input_path, "r", **open_kwargs) as f:
            for line in f:
                yield line

    iterator = iter_lines()
    if show_progress:
        with open(input_path, "r", **open_kwargs) as f:
            line_count = sum(1 for _ in f)
        iterator = tqdm(iter_lines(), total=line_count, desc=f"Packing {input_path.name}")

    for line in iterator:
        if not line.strip():
            continue
        record = json.loads(line)
        token_ids = record.get("input_ids")
        if token_ids is None:
            stats["invalid_records"] += 1
            continue
        if not isinstance(token_ids, list):
            stats["invalid_records"] += 1
            continue
        if not token_ids:
            stats["empty_records"] += 1
            continue
        if not all(isinstance(tid, int) for tid in token_ids):
            stats["invalid_records"] += 1
            continue

        stats["total_input_tokens"] += len(token_ids)
        buffer.extend(token_ids)
        while len(buffer) >= seq_len:
            chunk = buffer[:seq_len]
            sequences.append(chunk)
            buffer = buffer[stride:]

    if buffer and not drop_remainder:
        if len(buffer) < max(8, seq_len // 8):
            print(
                f"âš ï¸ Remainder nhá» ({len(buffer)} tokens). Padding gáº§n full sequence cÃ³ thá»ƒ gÃ¢y nhiá»…u."
            )
        padded = buffer + [pad_token_id] * (seq_len - len(buffer))
        sequences.append(padded[:seq_len])
        stats["padded_sequences"] += 1
    elif buffer and drop_remainder:
        stats["dropped_tokens"] += len(buffer)

    if not sequences:
        raise ValueError("KhÃ´ng táº¡o Ä‘Æ°á»£c sequence nÃ o. Kiá»ƒm tra seq_len/stride hoáº·c dá»¯ liá»‡u Ä‘áº§u vÃ o.")

    tensor = torch.tensor(sequences, dtype=torch.long)
    stats["total_output_sequences"] = tensor.size(0)
    return tensor, stats


def main() -> None:
    setup_encoding()
    args = parse_args()

    if not args.input.exists():
        raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file input: {args.input}")

    ensure_dir(args.output.parent)

    pad_token_id: Optional[int] = args.pad_token_id
    tokenizer_meta: Dict[str, str] = {}

    if pad_token_id is None and args.tokenizer_model:
        if spm is None:
            print(
                "âš ï¸ KhÃ´ng tÃ¬m tháº¥y sentencepiece. CÃ i sentencepiece hoáº·c truyá»n --pad-token-id."
            )
        else:
            sp = spm.SentencePieceProcessor(model_file=str(args.tokenizer_model))
            pad_id = sp.pad_id()
            unk_id = sp.unk_id()
            if pad_id >= 0:
                pad_token_id = pad_id
                tokenizer_meta["pad_id_source"] = "sentencepiece_pad"
            else:
                pad_token_id = unk_id
                tokenizer_meta["pad_id_source"] = "sentencepiece_unk"
                print("âš ï¸ Tokenizer khÃ´ng cÃ³ pad_id. Äang dÃ¹ng táº¡m unk_id Ä‘á»ƒ padding.")
            if pad_token_id == unk_id:
                print(
                    f"âš ï¸ pad_token_id={pad_token_id} trÃ¹ng unk_id. "
                    "HÃ£y cÃ¢n nháº¯c bá»• sung <pad> khi train tokenizer."
                )

    if pad_token_id is None:
        pad_token_id = 0
        tokenizer_meta["pad_id_source"] = "default_zero"
        print(
            "âš ï¸ pad_token_id khÃ´ng Ä‘Æ°á»£c cung cáº¥p. Äang fallback vá» 0 (thÆ°á»ng lÃ  <unk>). "
            "NÃªn truyá»n giÃ¡ trá»‹ rÃµ rÃ ng Ä‘á»ƒ trÃ¡nh nhiá»…u."
        )

    print(f"ğŸ” Packing {args.input} â†’ {args.output}")

    tensor, stats = pack_sequences(
        input_path=args.input,
        seq_len=args.seq_len,
        stride=args.stride,
        drop_remainder=args.drop_remainder,
        pad_token_id=pad_token_id,
        show_progress=args.show_progress,
    )

    meta = {
        "num_sequences": tensor.size(0),
        "seq_len": tensor.size(1),
        "stride": args.stride,
        "drop_remainder": args.drop_remainder,
        "pad_token_id": pad_token_id,
        "source": str(args.input),
        **stats,
        **tokenizer_meta,
    }

    torch.save({"input_ids": tensor, "meta": meta}, args.output)
    print(
        f"âœ… Done. Saved {tensor.size(0):,} sequences of length {tensor.size(1)} "
        f"â†’ {args.output}"
    )


if __name__ == "__main__":
    main()

