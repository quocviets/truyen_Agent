"""
Build SentencePiece tokenizer tá»« train split cho tiáº¿ng Viá»‡t.

Chiáº¿n lÆ°á»£c:
    1. Äá»c training/dataset/splits/train.jsonl (chá»‰ dÃ¹ng train Ä‘á»ƒ train tokenizer).
    2. Extract text tá»« má»—i paragraph, ghi vÃ o file táº¡m (1 dÃ²ng = 1 paragraph).
    3. Train SentencePiece tokenizer vá»›i vocab size 32k-50k (phÃ¹ há»£p model 1-3B).
    4. LÆ°u model + vocab vÃ o training/tokenizer/.
    5. Test tokenizer trÃªn sample text Ä‘á»ƒ verify.

YÃªu cáº§u:
    pip install sentencepiece
    python training/trainer/build_tokenizer.py \
        --input-jsonl training/dataset/splits/train.jsonl \
        --output-dir training/tokenizer \
        --vocab-size 32000 \
        --model-type bpe
"""

from __future__ import annotations

import argparse
import json
import tempfile
from pathlib import Path
from typing import List

import sentencepiece as spm

from .config import Paths, TokenizerConfig
from .utils import setup_encoding, save_json

# Setup encoding for Windows
setup_encoding()


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Build SentencePiece tokenizer tá»« train split")
    parser.add_argument(
        "--input-jsonl",
        type=Path,
        default=Paths.TRAIN_JSONL,
        help="File train.jsonl chá»©a paragraphs Ä‘á»ƒ train tokenizer"
    )
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Paths.TOKENIZER_DIR,
        help="ThÆ° má»¥c lÆ°u tokenizer model vÃ  vocab"
    )
    parser.add_argument(
        "--vocab-size",
        type=int,
        default=32000,
        help="Vocab size cho tokenizer (default: 32000, phÃ¹ há»£p model 1-3B)"
    )
    parser.add_argument(
        "--model-type",
        choices=["bpe", "unigram"],
        default="bpe",
        help="Loáº¡i SentencePiece model: bpe (Byte Pair Encoding) hoáº·c unigram (default: bpe)"
    )
    parser.add_argument(
        "--character-coverage",
        type=float,
        default=0.9995,
        help="Character coverage cho tiáº¿ng Viá»‡t (default: 0.9995, cao hÆ¡n tiáº¿ng Anh vÃ¬ cÃ³ dáº¥u)"
    )
    parser.add_argument(
        "--test-samples",
        type=int,
        default=5,
        help="Sá»‘ sample text Ä‘á»ƒ test tokenizer sau khi train (default: 5)"
    )
    return parser.parse_args()


def load_train_texts(input_jsonl: Path) -> List[str]:
    """
    Äá»c file train.jsonl vÃ  extract text tá»« má»—i paragraph.
    
    Args:
        input_jsonl: ÄÆ°á»ng dáº«n Ä‘áº¿n file train.jsonl
        
    Returns:
        List cÃ¡c Ä‘oáº¡n text (má»—i Ä‘oáº¡n = 1 paragraph)
    """
    from .utils import read_jsonl
    
    texts = []
    print(f"ğŸ“– Äang Ä‘á»c {input_jsonl}...")
    
    for record in read_jsonl(input_jsonl):
        text = record.get('text', '').strip()
        if text:
            texts.append(text)
    
    print(f"âœ… ÄÃ£ Ä‘á»c {len(texts):,} paragraphs")
    
    # Cáº£nh bÃ¡o náº¿u data quÃ¡ Ã­t
    if len(texts) < 10000:
        print(f"âš ï¸  Warning: Sá»‘ paragraph train hÆ¡i Ã­t ({len(texts):,}), tokenizer cÃ³ thá»ƒ khÃ´ng á»•n Ä‘á»‹nh.")
    
    return texts


def train_tokenizer(
    texts: List[str],
    output_dir: Path,
    vocab_size: int,
    model_type: str,
    character_coverage: float
) -> Path:
    """
    Train SentencePiece tokenizer tá»« danh sÃ¡ch texts.
    
    Args:
        texts: List cÃ¡c Ä‘oáº¡n text Ä‘á»ƒ train
        output_dir: ThÆ° má»¥c lÆ°u tokenizer
        vocab_size: KÃ­ch thÆ°á»›c vocab
        model_type: "bpe" hoáº·c "unigram"
        character_coverage: Character coverage (0.9995 cho tiáº¿ng Viá»‡t)
        
    Returns:
        ÄÆ°á»ng dáº«n Ä‘áº¿n file model Ä‘Ã£ train
    """
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Táº¡o file táº¡m chá»©a text (SentencePiece yÃªu cáº§u file input)
    print("ğŸ“ Äang táº¡o file táº¡m cho SentencePiece...")
    with tempfile.NamedTemporaryFile(
        mode='w',
        encoding='utf-8',
        suffix='.txt',
        delete=False
    ) as tmp_file:
        tmp_path = Path(tmp_file.name)
        for text in texts:
            # Ghi má»—i paragraph trÃªn 1 dÃ²ng
            tmp_file.write(text + '\n')
    
    print(f"âœ… ÄÃ£ ghi {len(texts):,} paragraphs vÃ o file táº¡m")
    
    # ÄÆ°á»ng dáº«n output model
    model_prefix = output_dir / "sp_model"
    model_path = model_prefix.with_suffix('.model')
    vocab_path = model_prefix.with_suffix('.vocab')
    
    print(f"\nğŸ”§ Äang train SentencePiece tokenizer...")
    print(f"   Model type: {model_type}")
    print(f"   Vocab size: {vocab_size:,}")
    print(f"   Character coverage: {character_coverage}")
    print(f"   Input file: {tmp_path}")
    print(f"   Output model: {model_path}")
    
    # Train SentencePiece
    spm.SentencePieceTrainer.train(
        input=str(tmp_path),
        model_prefix=str(model_prefix),
        vocab_size=vocab_size,
        model_type=model_type,
        character_coverage=character_coverage,
        # ThÃªm cÃ¡c tham sá»‘ tá»‘i Æ°u cho tiáº¿ng Viá»‡t
        normalization_rule_name='nmt_nfkc_cf',  # Normalize Unicode
        remove_extra_whitespaces=True,
        # Chá»‰ thÃªm cÃ¡c special tokens khÃ´ng cÃ³ sáºµn trong SentencePiece
        # (<unk>, <s>, </s> Ä‘Ã£ cÃ³ sáºµn, khÃ´ng cáº§n khai bÃ¡o láº¡i)
        user_defined_symbols=['<pad>', '<mask>'],
        # Shuffle input Ä‘á»ƒ training á»•n Ä‘á»‹nh hÆ¡n
        shuffle_input_sentence=True,
        # Sá»‘ threads (1 = single thread, hoáº·c bá» Ä‘á»ƒ dÃ¹ng default)
        num_threads=1,
        # Input sentence size limit (0 = no limit)
        input_sentence_size=0,
        # Tá»‘i Æ°u cho text dÃ i (truyá»‡n)
        max_sentence_length=4192,
    )
    
    # XÃ³a file táº¡m
    tmp_path.unlink()
    
    print(f"âœ… Tokenizer Ä‘Ã£ Ä‘Æ°á»£c train vÃ  lÆ°u táº¡i:")
    print(f"   Model: {model_path}")
    print(f"   Vocab: {vocab_path}")
    
    return model_path


def test_tokenizer(model_path: Path, texts: List[str], num_samples: int = 5):
    """
    Test tokenizer trÃªn má»™t sá»‘ sample text Ä‘á»ƒ verify.
    
    Args:
        model_path: ÄÆ°á»ng dáº«n Ä‘áº¿n file model
        texts: List cÃ¡c Ä‘oáº¡n text Ä‘á»ƒ test
        num_samples: Sá»‘ sample Ä‘á»ƒ test
    """
    print(f"\nğŸ§ª Äang test tokenizer trÃªn {num_samples} samples...")
    
    sp = spm.SentencePieceProcessor()
    sp.load(str(model_path))
    
    # Láº¥y má»™t sá»‘ sample ngáº«u nhiÃªn
    import random
    random.seed(42)
    samples = random.sample(texts, min(num_samples, len(texts)))
    
    for i, text in enumerate(samples, 1):
        print(f"\n--- Sample {i} ---")
        print(f"Original text (first 200 chars):")
        print(f"  {text[:200]}...")
        
        # Encode
        tokens = sp.encode(text, out_type=str)
        token_ids = sp.encode(text, out_type=int)
        
        print(f"\nTokens ({len(tokens)} tokens):")
        print(f"  {tokens[:20]}..." if len(tokens) > 20 else f"  {tokens}")
        
        # Decode Ä‘á»ƒ verify
        decoded = sp.decode(tokens)
        print(f"\nDecoded (first 200 chars):")
        print(f"  {decoded[:200]}...")
        
        # Kiá»ƒm tra round-trip
        if text.strip() == decoded.strip():
            print("  âœ… Round-trip OK")
        else:
            print("  âš ï¸  Round-trip cÃ³ khÃ¡c biá»‡t (cÃ³ thá»ƒ do normalization)")
        
        # Thá»‘ng kÃª
        print(f"\nStats:")
        print(f"  Original length: {len(text)} chars")
        print(f"  Token count: {len(tokens)}")
        print(f"  Compression ratio: {len(text) / len(tokens):.2f} chars/token")


def save_tokenizer_info(
    output_dir: Path,
    vocab_size: int,
    model_type: str,
    character_coverage: float,
    total_texts: int,
    model_path: Path
):
    """
    LÆ°u metadata vá» tokenizer vÃ o file JSON.
    
    Args:
        output_dir: ThÆ° má»¥c output
        vocab_size: Vocab size
        model_type: Model type
        character_coverage: Character coverage
        total_texts: Tá»•ng sá»‘ texts Ä‘Ã£ train
        model_path: ÄÆ°á»ng dáº«n Ä‘áº¿n model
    """
    info = {
        "tokenizer_type": "sentencepiece",
        "model_type": model_type,
        "vocab_size": vocab_size,
        "character_coverage": character_coverage,
        "training_samples": total_texts,
        "model_path": str(model_path),
        "vocab_path": str(model_path.with_suffix('.vocab')),
        "notes": [
            "Tokenizer Ä‘Æ°á»£c train tá»« train split (90% data)",
            "PhÃ¹ há»£p cho model 1-3B parameters",
            "Sá»­ dá»¥ng BPE vá»›i character coverage cao cho tiáº¿ng Viá»‡t"
        ]
    }
    
    info_path = Paths.TOKENIZER_INFO_JSON
    save_json(info_path, info)
    
    print(f"\nğŸ“„ ÄÃ£ lÆ°u tokenizer info: {info_path}")


def main():
    args = parse_args()
    
    input_jsonl = Path(args.input_jsonl)
    output_dir = Path(args.output_dir)
    
    if not input_jsonl.exists():
        print(f"âŒ File khÃ´ng tá»“n táº¡i: {input_jsonl}")
        return 1
    
    # Load texts tá»« train split
    texts = load_train_texts(input_jsonl)
    
    if not texts:
        print("âŒ KhÃ´ng cÃ³ text nÃ o Ä‘á»ƒ train tokenizer")
        return 1
    
    # Train tokenizer
    model_path = train_tokenizer(
        texts=texts,
        output_dir=output_dir,
        vocab_size=args.vocab_size,
        model_type=args.model_type,
        character_coverage=args.character_coverage
    )
    
    # Test tokenizer
    test_tokenizer(model_path, texts, num_samples=args.test_samples)
    
    # LÆ°u metadata
    save_tokenizer_info(
        output_dir=output_dir,
        vocab_size=args.vocab_size,
        model_type=args.model_type,
        character_coverage=args.character_coverage,
        total_texts=len(texts),
        model_path=model_path
    )
    
    print(f"\nâœ… HoÃ n táº¥t! Tokenizer Ä‘Ã£ sáºµn sÃ ng táº¡i: {output_dir}")
    print(f"\nğŸ’¡ Äá»ƒ sá»­ dá»¥ng tokenizer trong code:")
    print(f"   import sentencepiece as spm")
    print(f"   sp = spm.SentencePieceProcessor()")
    print(f"   sp.load('{model_path}')")
    print(f"   tokens = sp.encode('VÄƒn báº£n tiáº¿ng Viá»‡t')")
    
    return 0


if __name__ == "__main__":
    exit(main())


