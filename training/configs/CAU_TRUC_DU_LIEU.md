# ğŸ“ Cáº¤U TRÃšC Dá»® LIá»†U - CHI TIáº¾T Tá»ªNG BÆ¯á»šC

**Má»¥c Ä‘Ã­ch:** XÃ¡c Ä‘á»‹nh cáº¥u trÃºc dá»¯ liá»‡u á»Ÿ má»—i bÆ°á»›c cá»§a pipeline, tá»« raw Ä‘áº¿n training-ready.

---

## ğŸ“‹ Tá»”NG QUAN

### Pipeline Flow:
```
Raw Data â†’ Preprocessed â†’ Tokenized â†’ Splits â†’ Training
```

### Má»—i bÆ°á»›c cÃ³ cáº¥u trÃºc riÃªng:
1. **Raw:** Chapter files riÃªng láº»
2. **Preprocessed:** Text Ä‘Ã£ lÃ m sáº¡ch (combined hoáº·c separate)
3. **Tokenized:** Text Ä‘Ã£ tokenize thÃ nh tokens
4. **Splits:** Train/Val/Test splits
5. **Training:** Dataset format cho model

---

## ğŸ”¹ BÆ¯á»šC 1: RAW DATA

### Cáº¥u trÃºc hiá»‡n táº¡i:
```
training/dataset/raw/truyenmoiii_output/
â”œâ”€â”€ van-co-than-de/
â”‚   â”œâ”€â”€ chapter_1.txt
â”‚   â”œâ”€â”€ chapter_2.txt
â”‚   â”œâ”€â”€ ...
â”‚   â”œâ”€â”€ chapter_4550.txt
â”‚   â””â”€â”€ novel_summary.json
â”œâ”€â”€ than-dao-de-ton/
â”‚   â”œâ”€â”€ chapter_1.txt
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ novel_summary.json
â””â”€â”€ ...
```

### Format file:
- **Chapter files:** Plain text, UTF-8 encoding
- **novel_summary.json:** Metadata (URLs, chapter list, ...)

### Äáº·c Ä‘iá»ƒm:
- Má»—i chapter lÃ  má»™t file riÃªng
- KhÃ´ng cÃ³ preprocessing
- CÃ³ thá»ƒ cÃ²n HTML tags, control characters

---

## ğŸ”¹ BÆ¯á»šC 2: PREPROCESSED DATA

### Cáº¥u trÃºc Ä‘á» xuáº¥t:

#### **Option A: COMBINED (1 file/truyá»‡n) - â­ Äá»€ XUáº¤T**

```
training/dataset/preprocessed/
â”œâ”€â”€ van-co-than-de_preprocessed.txt
â”œâ”€â”€ van-co-than-de_metadata.json
â”œâ”€â”€ than-dao-de-ton_preprocessed.txt
â”œâ”€â”€ than-dao-de-ton_metadata.json
â”œâ”€â”€ ...
â””â”€â”€ preprocessing_summary.json
```

**Format file:**

**1. `{novel_name}_preprocessed.txt`:**
```
Äoáº¡n vÄƒn 1 tá»« chapter 1...

Äoáº¡n vÄƒn 2 tá»« chapter 1...

Äoáº¡n vÄƒn 1 tá»« chapter 2...

...
```

- Má»—i paragraph cÃ¡ch nhau bá»Ÿi 2 newlines (`\n\n`)
- KhÃ´ng cÃ³ separator giá»¯a cÃ¡c chapter (chá»‰ dá»±a vÃ o paragraph breaks)
- Encoding: UTF-8

**2. `{novel_name}_metadata.json`:**
```json
{
  "novel_name": "van-co-than-de",
  "total_chapters": 4550,
  "processed_chapters": 4543,
  "filtered_chapters": 7,
  "total_paragraphs": 125000,
  "total_chars": 8500000,
  "total_bytes": 8500000,
  "avg_chars_per_chapter": 1870,
  "avg_chars_per_paragraph": 68,
  "preprocessing_config": {
    "min_chapter_length": 500,
    "min_ratio": 0.1
  }
}
```

**3. `preprocessing_summary.json`:**
```json
{
  "preprocessing_config": {
    "min_chapter_length": 500,
    "min_ratio": 0.1
  },
  "statistics": {
    "total_novels": 11,
    "total_chapters": 19966,
    "processed_chapters": 19959,
    "filtered_chapters": 7,
    "total_chars": 50000000,
    "total_bytes": 50000000
  },
  "novels": {
    "van-co-than-de": { ... },
    "than-dao-de-ton": { ... },
    ...
  }
}
```

**Æ¯u Ä‘iá»ƒm:**
- ÄÆ¡n giáº£n, dá»… quáº£n lÃ½
- Dá»… Ä‘á»c toÃ n bá»™ truyá»‡n
- PhÃ¹ há»£p cho training (cÃ³ thá»ƒ Ä‘á»c tuáº§n tá»±)

**NhÆ°á»£c Ä‘iá»ƒm:**
- File lá»›n (cÃ³ thá»ƒ vÃ i trÄƒm MB)
- KhÃ³ xá»­ lÃ½ song song

---

#### **Option B: SEPARATE (Nhiá»u file/truyá»‡n)**

```
training/dataset/preprocessed/
â”œâ”€â”€ van-co-than-de/
â”‚   â”œâ”€â”€ chapter_00001.txt
â”‚   â”œâ”€â”€ chapter_00002.txt
â”‚   â”œâ”€â”€ ...
â”‚   â”œâ”€â”€ chapter_04550.txt
â”‚   â””â”€â”€ metadata.json
â”œâ”€â”€ than-dao-de-ton/
â”‚   â”œâ”€â”€ chapter_00001.txt
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ metadata.json
â”œâ”€â”€ ...
â””â”€â”€ preprocessing_summary.json
```

**Format file:**

**1. `chapter_{number:05d}.txt`:**
```
Äoáº¡n vÄƒn 1...

Äoáº¡n vÄƒn 2...

...
```

- Má»—i file lÃ  má»™t chapter Ä‘Ã£ preprocessed
- Format: `chapter_00001.txt`, `chapter_00002.txt`, ...
- Encoding: UTF-8

**2. `metadata.json` (trong má»—i folder):**
```json
{
  "novel_name": "van-co-than-de",
  "total_chapters": 4550,
  "processed_chapters": 4543,
  "filtered_chapters": 7,
  "total_paragraphs": 125000,
  "total_chars": 8500000,
  "total_bytes": 8500000
}
```

**Æ¯u Ä‘iá»ƒm:**
- Dá»… xá»­ lÃ½ song song
- Dá»… quáº£n lÃ½ tá»«ng chapter
- File nhá», dá»… load

**NhÆ°á»£c Ä‘iá»ƒm:**
- Nhiá»u file, khÃ³ quáº£n lÃ½
- Tá»‘n I/O khi Ä‘á»c nhiá»u file

---

### **Khuyáº¿n nghá»‹:** Option A (COMBINED) - â­

**LÃ½ do:**
- ÄÆ¡n giáº£n, dá»… quáº£n lÃ½
- PhÃ¹ há»£p cho training (Ä‘á»c tuáº§n tá»±)
- Dá»… implement

---

## ğŸ”¹ BÆ¯á»šC 3: TOKENIZED DATA

### Cáº¥u trÃºc Ä‘á» xuáº¥t:

```
training/dataset/tokenized/
â”œâ”€â”€ van-co-than-de_tokenized.pt
â”œâ”€â”€ van-co-than-de_tokenized.json
â”œâ”€â”€ than-dao-de-ton_tokenized.pt
â”œâ”€â”€ than-dao-de-ton_tokenized.json
â”œâ”€â”€ ...
â”œâ”€â”€ tokenizer_config.json
â””â”€â”€ tokenization_summary.json
```

### Format file:

**1. `{novel_name}_tokenized.pt` (PyTorch format):**
- Tensor chá»©a token IDs
- Shape: `[total_tokens]`
- Dtype: `torch.int32` hoáº·c `torch.int64`
- **VÃ­ dá»¥:** `[101, 234, 567, ..., 102]`

**2. `{novel_name}_tokenized.json` (Metadata):**
```json
{
  "novel_name": "van-co-than-de",
  "total_tokens": 2000000,
  "total_chars": 8500000,
  "vocab_size": 50000,
  "tokenizer_type": "BPE",
  "special_tokens": {
    "bos": 101,
    "eos": 102,
    "pad": 0,
    "unk": 1
  },
  "chunk_info": [
    {
      "chunk_id": 0,
      "start_token": 0,
      "end_token": 512,
      "source_paragraph": 0
    },
    ...
  ]
}
```

**3. `tokenizer_config.json`:**
```json
{
  "tokenizer_type": "BPE",
  "vocab_size": 50000,
  "model_file": "tokenizer.model",
  "special_tokens": {
    "bos": "<|beginoftext|>",
    "eos": "<|endoftext|>",
    "pad": "<|pad|>",
    "unk": "<|unk|>"
  },
  "added_tokens": []
}
```

**4. `tokenization_summary.json`:**
```json
{
  "total_novels": 11,
  "total_tokens": 20000000,
  "vocab_size": 50000,
  "avg_tokens_per_chapter": 1000,
  "tokenizer_config": { ... }
}
```

---

## ğŸ”¹ BÆ¯á»šC 4: SPLITS (Train/Val/Test)

### Cáº¥u trÃºc Ä‘á» xuáº¥t:

```
training/dataset/splits/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ train_data.pt
â”‚   â”œâ”€â”€ train_metadata.json
â”‚   â””â”€â”€ train_indices.json
â”œâ”€â”€ val/
â”‚   â”œâ”€â”€ val_data.pt
â”‚   â”œâ”€â”€ val_metadata.json
â”‚   â””â”€â”€ val_indices.json
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ test_data.pt
â”‚   â”œâ”€â”€ test_metadata.json
â”‚   â””â”€â”€ test_indices.json
â””â”€â”€ splits_config.json
```

### Format file:

**1. `{split}_data.pt` (PyTorch format):**
- Tensor chá»©a token IDs cho split Ä‘Ã³
- Shape: `[num_samples, sequence_length]`
- Dtype: `torch.int32` hoáº·c `torch.int64`
- **VÃ­ dá»¥:** `[[101, 234, ..., 102], [101, 567, ..., 102], ...]`

**2. `{split}_metadata.json`:**
```json
{
  "split": "train",
  "num_samples": 150000,
  "sequence_length": 512,
  "total_tokens": 76800000,
  "source_novels": [
    "van-co-than-de",
    "than-dao-de-ton",
    ...
  ]
}
```

**3. `{split}_indices.json`:**
```json
{
  "indices": [
    {
      "sample_id": 0,
      "novel": "van-co-than-de",
      "chunk_id": 0,
      "paragraph_id": 0
    },
    ...
  ]
}
```

**4. `splits_config.json`:**
```json
{
  "train_ratio": 0.8,
  "val_ratio": 0.1,
  "test_ratio": 0.1,
  "split_method": "sequential",  // hoáº·c "random"
  "sequence_length": 512,
  "stride": 256  // overlap cho sliding window
}
```

---

## ğŸ”¹ BÆ¯á»šC 5: TRAINING DATASET

### Cáº¥u trÃºc Ä‘á» xuáº¥t:

```
training/dataset/training/
â”œâ”€â”€ dataset.pt
â”œâ”€â”€ dataset_metadata.json
â””â”€â”€ dataset_config.json
```

### Format file:

**1. `dataset.pt` (PyTorch Dataset):**
- CÃ³ thá»ƒ dÃ¹ng `torch.utils.data.Dataset`
- Hoáº·c lÆ°u trá»±c tiáº¿p tensor

**2. `dataset_metadata.json`:**
```json
{
  "train_samples": 150000,
  "val_samples": 15000,
  "test_samples": 15000,
  "sequence_length": 512,
  "vocab_size": 50000,
  "total_tokens": 90000000
}
```

**3. `dataset_config.json`:**
```json
{
  "batch_size": 32,
  "sequence_length": 512,
  "vocab_size": 50000,
  "data_loader_config": {
    "shuffle": true,
    "num_workers": 4,
    "pin_memory": true
  }
}
```

---

## ğŸ“Š Tá»”NG Káº¾T Cáº¤U TRÃšC

### **Cáº¥u trÃºc Ä‘áº§y Ä‘á»§:**

```
training/dataset/
â”œâ”€â”€ raw/
â”‚   â””â”€â”€ truyenmoiii_output/
â”‚       â”œâ”€â”€ van-co-than-de/
â”‚       â”‚   â”œâ”€â”€ chapter_*.txt
â”‚       â”‚   â””â”€â”€ novel_summary.json
â”‚       â””â”€â”€ ...
â”œâ”€â”€ preprocessed/
â”‚   â”œâ”€â”€ {novel}_preprocessed.txt
â”‚   â”œâ”€â”€ {novel}_metadata.json
â”‚   â””â”€â”€ preprocessing_summary.json
â”œâ”€â”€ tokenized/
â”‚   â”œâ”€â”€ {novel}_tokenized.pt
â”‚   â”œâ”€â”€ {novel}_tokenized.json
â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â””â”€â”€ tokenization_summary.json
â”œâ”€â”€ splits/
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ train_data.pt
â”‚   â”‚   â”œâ”€â”€ train_metadata.json
â”‚   â”‚   â””â”€â”€ train_indices.json
â”‚   â”œâ”€â”€ val/
â”‚   â””â”€â”€ test/
â””â”€â”€ training/
    â”œâ”€â”€ dataset.pt
    â”œâ”€â”€ dataset_metadata.json
    â””â”€â”€ dataset_config.json
```

---

## ğŸ”„ QUY TRÃŒNH CHUYá»‚N Äá»”I

### **Raw â†’ Preprocessed:**
```
chapter_*.txt â†’ clean_text() â†’ paragraphs â†’ {novel}_preprocessed.txt
```

### **Preprocessed â†’ Tokenized:**
```
{novel}_preprocessed.txt â†’ tokenizer.encode() â†’ token_ids â†’ {novel}_tokenized.pt
```

### **Tokenized â†’ Splits:**
```
{novel}_tokenized.pt â†’ chunking (sliding window) â†’ train/val/test splits
```

### **Splits â†’ Training:**
```
train/val/test splits â†’ PyTorch Dataset â†’ DataLoader
```

---

## ğŸ“ LÆ¯U Ã QUAN TRá»ŒNG

1. **Encoding:** Táº¥t cáº£ file text Ä‘á»u UTF-8
2. **Format:** Metadata luÃ´n lÃ  JSON
3. **Tensor:** DÃ¹ng PyTorch format (.pt) cho tokenized data
4. **Naming:** Consistent naming convention
5. **Versioning:** CÃ³ thá»ƒ thÃªm version number náº¿u cáº§n

---

## ğŸ¯ Káº¾T LUáº¬N

**Cáº¥u trÃºc Ä‘á» xuáº¥t:**
- **Raw:** Chapter files riÃªng láº»
- **Preprocessed:** Combined format (1 file/truyá»‡n) - â­
- **Tokenized:** PyTorch tensor + JSON metadata
- **Splits:** Train/Val/Test vá»›i metadata
- **Training:** PyTorch Dataset format

**LÃ½ do:**
- ÄÆ¡n giáº£n, dá»… quáº£n lÃ½
- PhÃ¹ há»£p cho training
- Dá»… debug vÃ  validate


