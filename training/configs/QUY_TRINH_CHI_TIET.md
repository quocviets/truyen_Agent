# ğŸ“‹ QUY TRÃŒNH PIPELINE CHI TIáº¾T - Tá»ªNG BÆ¯á»šC Cá»¤ THá»‚

## ğŸ¯ Má»¤C TIÃŠU Tá»”NG QUAN

XÃ¢y dá»±ng Language Model Ä‘á»ƒ generate text tiáº¿p theo tá»« prompt, train trÃªn dá»¯ liá»‡u truyá»‡n tiáº¿ng Viá»‡t.

---

# ğŸ”¹ BÆ¯á»šC 1: TIá»€N Xá»¬ LÃ (PREPROCESSING)

## ğŸ“¥ INPUT
```
truyenmoiii_output/
â”œâ”€â”€ chapter_1.txt          (VÃ­ dá»¥: "Äá»c Tá»« Äáº§u\n\nNáº¯ng áº¥m xuyÃªn qua...")
â”œâ”€â”€ chapter_2.txt
â”œâ”€â”€ chapter_3.txt
â”œâ”€â”€ ...
â”œâ”€â”€ chapter_580.txt
â””â”€â”€ novel_summary.json    (Metadata: danh sÃ¡ch chÆ°Æ¡ng, URL, ...)
```

## ğŸ”„ QUY TRÃŒNH CHI TIáº¾T

### BÆ°á»›c 1.1: Äá»c dá»¯ liá»‡u
**Má»¥c Ä‘Ã­ch:** Load táº¥t cáº£ file chapter vÃ o memory

**CÃ´ng viá»‡c:**
1. QuÃ©t thÆ° má»¥c `truyenmoiii_output/`
2. TÃ¬m táº¥t cáº£ file cÃ³ pattern `chapter_*.txt`
3. Sáº¯p xáº¿p theo sá»‘ thá»© tá»± (chapter_1, chapter_2, ...)
4. Äá»c tá»«ng file vá»›i encoding UTF-8
5. LÆ°u ná»™i dung vÃ o list: `raw_texts = [text1, text2, ...]`

**VÃ­ dá»¥:**
```python
# TÃ¬m files
files = ["chapter_1.txt", "chapter_2.txt", ..., "chapter_580.txt"]

# Äá»c ná»™i dung
raw_texts = []
for file in files:
    with open(file, 'r', encoding='utf-8') as f:
        content = f.read()
        raw_texts.append(content)
```

**Output:** List cÃ¡c string chá»©a ná»™i dung raw text

---

### BÆ°á»›c 1.2: LÃ m sáº¡ch text cÆ¡ báº£n
**Má»¥c Ä‘Ã­ch:** XÃ³a cÃ¡c kÃ½ tá»± khÃ´ng cáº§n thiáº¿t, chuáº©n hÃ³a format

**CÃ´ng viá»‡c:**

**1.2.1. XÃ³a Control Characters**
- TÃ¬m vÃ  xÃ³a cÃ¡c kÃ½ tá»± invisible: `\x00-\x08`, `\x0b-\x0c`, `\x0e-\x1f`, `\x7f-\x9f`
- **VÃ­ dá»¥:**
  - Input: `"Text\x00\x01\x02"`
  - Output: `"Text"`

**1.2.2. Chuáº©n hÃ³a khoáº£ng tráº¯ng**
- Nhiá»u space liÃªn tiáº¿p â†’ 1 space
- Nhiá»u tab â†’ 1 space
- **VÃ­ dá»¥:**
  - Input: `"CÃ³    nhiá»u     space"`
  - Output: `"CÃ³ nhiá»u space"`

**1.2.3. Chuáº©n hÃ³a xuá»‘ng dÃ²ng**
- Nhiá»u newline liÃªn tiáº¿p (3+) â†’ 2 newlines (giá»¯ paragraph break)
- **VÃ­ dá»¥:**
  - Input: `"Äoáº¡n 1\n\n\n\n\nÄoáº¡n 2"`
  - Output: `"Äoáº¡n 1\n\nÄoáº¡n 2"`

**1.2.4. XÃ³a dÃ²ng quÃ¡ ngáº¯n**
- CÃ¡c dÃ²ng cÃ³ < 10 kÃ½ tá»± (cÃ³ thá»ƒ lÃ  lá»—i format) â†’ xÃ³a
- **VÃ­ dá»¥:**
  - Input: `"Äoáº¡n vÄƒn dÃ i\n\n\nNgáº¯n\n\nÄoáº¡n khÃ¡c"`
  - Output: `"Äoáº¡n vÄƒn dÃ i\n\nÄoáº¡n khÃ¡c"`

**1.2.5. XÃ³a kÃ½ tá»± Ä‘áº·c biá»‡t khÃ´ng cáº§n thiáº¿t**
- Giá»¯ láº¡i:
  - Chá»¯ cÃ¡i tiáº¿ng Viá»‡t (a-z, A-Z, Ã -á»¹, Ã€-á»¸)
  - Sá»‘ (0-9)
  - Dáº¥u cÃ¢u tiáº¿ng Viá»‡t: `. , ! ? ; : ( ) [ ] { } " ' - â€“ â€” â€¦`
  - Khoáº£ng tráº¯ng, xuá»‘ng dÃ²ng
- XÃ³a: CÃ¡c kÃ½ tá»± khÃ¡c (emoji, symbol Ä‘áº·c biá»‡t, ...)
- **VÃ­ dá»¥:**
  - Input: `"Nham Kiá»u ğŸ˜Š Ä‘ang Ä‘á»©ng @#$%"`
  - Output: `"Nham Kiá»u Ä‘ang Ä‘á»©ng"`

**Output:** Text Ä‘Ã£ Ä‘Æ°á»£c lÃ m sáº¡ch cÆ¡ báº£n

---

### BÆ°á»›c 1.3: Chia thÃ nh Ä‘oáº¡n vÄƒn
**Má»¥c Ä‘Ã­ch:** Chia text thÃ nh cÃ¡c Ä‘oáº¡n vÄƒn há»£p lá»‡ Ä‘á»ƒ training

**CÃ´ng viá»‡c:**

**1.3.1. Chia theo paragraph break**
- TÃ¡ch text theo pattern: `\n\s*\n` (2 newlines liÃªn tiáº¿p)
- **VÃ­ dá»¥:**
  - Input: `"Äoáº¡n 1\n\nÄoáº¡n 2\n\nÄoáº¡n 3"`
  - Output: `["Äoáº¡n 1", "Äoáº¡n 2", "Äoáº¡n 3"]`

**1.3.2. Lá»c Ä‘oáº¡n há»£p lá»‡**
- Kiá»ƒm tra Ä‘á»™ dÃ i má»—i Ä‘oáº¡n:
  - **QuÃ¡ ngáº¯n (< 50 kÃ½ tá»±):** Bá» qua (cÃ³ thá»ƒ lÃ  tiÃªu Ä‘á», lá»—i format)
  - **Há»£p lá»‡ (50-2000 kÃ½ tá»±):** Giá»¯ láº¡i
  - **QuÃ¡ dÃ i (> 2000 kÃ½ tá»±):** Chia nhá» (xem BÆ°á»›c 1.3.3)

**VÃ­ dá»¥:**
```python
paragraphs = [
    "Äoáº¡n ngáº¯n",                    # < 50 â†’ Bá»
    "Äoáº¡n vÄƒn dÃ i há»£p lá»‡...",       # 50-2000 â†’ GIá»®
    "Äoáº¡n ráº¥t dÃ i..." * 1000        # > 2000 â†’ CHIA NHá»
]
```

**1.3.3. Chia Ä‘oáº¡n dÃ i thÃ nh chunks**
- Náº¿u Ä‘oáº¡n > 2000 kÃ½ tá»± â†’ chia thÃ nh nhiá»u chunks
- Chia theo cÃ¢u (dáº¥u cháº¥m, cháº¥m há»i, cháº¥m than)
- Má»—i chunk tá»‘i Ä‘a ~512 tokens (Æ°á»›c tÃ­nh: 1 token â‰ˆ 4 kÃ½ tá»± tiáº¿ng Viá»‡t)
- **VÃ­ dá»¥:**
  - Input: Äoáº¡n 3000 kÃ½ tá»±
  - Output: Chunk 1 (2000 kÃ½ tá»±), Chunk 2 (1000 kÃ½ tá»±)

**Output:** List cÃ¡c Ä‘oáº¡n vÄƒn há»£p lá»‡: `valid_paragraphs = [para1, para2, ...]`

---

### BÆ°á»›c 1.4: LÆ°u káº¿t quáº£
**Má»¥c Ä‘Ã­ch:** LÆ°u dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½ Ä‘á»ƒ sá»­ dá»¥ng á»Ÿ bÆ°á»›c sau

**CÃ´ng viá»‡c:**

**1.4.1. Táº¡o thÆ° má»¥c output**
- Táº¡o `data/preprocessed/` náº¿u chÆ°a cÃ³

**1.4.2. LÆ°u dá»¯ liá»‡u**
- File: `all_novels_preprocessed.json`
- Format:
```json
{
  "novel_name": "all_novels",
  "num_paragraphs": 15000,
  "paragraphs": [
    "Äoáº¡n vÄƒn 1...",
    "Äoáº¡n vÄƒn 2...",
    ...
  ]
}
```

**1.4.3. LÆ°u thá»‘ng kÃª**
- File: `preprocessing_summary.json`
- Format:
```json
{
  "total_novels": 1,
  "total_paragraphs": 15000,
  "total_characters": 5000000,
  "avg_paragraph_length": 333
}
```

## ğŸ“¤ OUTPUT
```
data/preprocessed/
â”œâ”€â”€ all_novels_preprocessed.json    (Dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½)
â””â”€â”€ preprocessing_summary.json      (Thá»‘ng kÃª)
```

---

# ğŸ”¹ BÆ¯á»šC 2: CHUáº¨N Bá»Š DATASET (DATA PREPARATION)

## ğŸ“¥ INPUT
```
data/preprocessed/
â””â”€â”€ all_novels_preprocessed.json
```

## ğŸ”„ QUY TRÃŒNH CHI TIáº¾T

### BÆ°á»›c 2.1: Load dá»¯ liá»‡u Ä‘Ã£ preprocess
**Má»¥c Ä‘Ã­ch:** Äá»c dá»¯ liá»‡u tá»« BÆ°á»›c 1

**CÃ´ng viá»‡c:**
1. Äá»c file `all_novels_preprocessed.json`
2. Extract list paragraphs: `paragraphs = data['paragraphs']`
3. **VÃ­ dá»¥:**
   - Input: File JSON vá»›i 15,000 paragraphs
   - Output: `paragraphs = ["para1", "para2", ..., "para15000"]`

**Output:** List cÃ¡c paragraphs (strings)

---

### BÆ°á»›c 2.2: XÃ¢y dá»±ng Vocabulary
**Má»¥c Ä‘Ã­ch:** Táº¡o tá»« Ä‘iá»ƒn mapping tá»« â†’ token ID

**CÃ´ng viá»‡c:**

**2.2.1. Äáº¿m táº§n suáº¥t tá»«**
- Chia má»—i paragraph thÃ nh tá»« (split theo space)
- Äáº¿m sá»‘ láº§n xuáº¥t hiá»‡n cá»§a má»—i tá»« trong toÃ n bá»™ dataset
- **VÃ­ dá»¥:**
  ```python
  word_counts = {
    "Nham": 500,
    "Kiá»u": 450,
    "Ä‘ang": 300,
    "Ä‘á»©ng": 200,
    ...
  }
  ```

**2.2.2. Láº¥y top N tá»« phá»• biáº¿n**
- Sáº¯p xáº¿p theo táº§n suáº¥t giáº£m dáº§n
- Láº¥y top 50,000 tá»« (trá»« special tokens)
- **VÃ­ dá»¥:**
  ```python
  top_words = [
    ("Nham", 500),
    ("Kiá»u", 450),
    ("Ä‘ang", 300),
    ...
  ]  # Top 50,000 tá»«
  ```

**2.2.3. Táº¡o Special Tokens**
- `<UNK>` (ID: 0): Tá»« khÃ´ng cÃ³ trong vocabulary
- `<PAD>` (ID: 1): Padding token
- `<BOS>` (ID: 2): Beginning of sequence
- `<EOS>` (ID: 3): End of sequence

**2.2.4. Táº¡o Mapping**
- `word_to_id`: Tá»« â†’ Token ID
- `id_to_word`: Token ID â†’ Tá»«
- **VÃ­ dá»¥:**
  ```python
  word_to_id = {
    "<UNK>": 0,
    "<PAD>": 1,
    "<BOS>": 2,
    "<EOS>": 3,
    "Nham": 4,
    "Kiá»u": 5,
    ...
  }
  
  id_to_word = {
    0: "<UNK>",
    1: "<PAD>",
    2: "<BOS>",
    3: "<EOS>",
    4: "Nham",
    5: "Kiá»u",
    ...
  }
  ```

**Output:** Vocabulary vá»›i 50,000+ tokens

---

### BÆ°á»›c 2.3: Táº¡o Tokenizer
**Má»¥c Ä‘Ã­ch:** CÃ´ng cá»¥ encode/decode text â†” tokens

**CÃ´ng viá»‡c:**

**2.3.1. Implement encode()**
- Input: Text string
- Output: List of token IDs
- **VÃ­ dá»¥:**
  ```python
  text = "Nham Kiá»u Ä‘ang Ä‘á»©ng"
  token_ids = [4, 5, 300, 200]  # [Nham, Kiá»u, Ä‘ang, Ä‘á»©ng]
  ```

**2.3.2. Implement decode()**
- Input: List of token IDs
- Output: Text string
- **VÃ­ dá»¥:**
  ```python
  token_ids = [4, 5, 300, 200]
  text = "Nham Kiá»u Ä‘ang Ä‘á»©ng"
  ```

**2.3.3. Há»— trá»£ padding/truncation**
- Náº¿u text quÃ¡ dÃ i â†’ truncate Ä‘áº¿n max_length
- Náº¿u text quÃ¡ ngáº¯n â†’ pad vá»›i `<PAD>` token
- **VÃ­ dá»¥:**
  ```python
  # Truncate
  text = "Ráº¥t dÃ i..." * 100
  tokens = encode(text, max_length=512)  # Chá»‰ láº¥y 512 tokens Ä‘áº§u
  
  # Padding
  text = "Ngáº¯n"
  tokens = encode(text, max_length=512)  # [tokens..., 1, 1, 1, ...] (pad Ä‘áº¿n 512)
  ```

**Output:** Tokenizer object cÃ³ thá»ƒ encode/decode

---

### BÆ°á»›c 2.4: Táº¡o Samples
**Má»¥c Ä‘Ã­ch:** Chuyá»ƒn paragraphs thÃ nh samples cÃ³ Ä‘á»™ dÃ i cá»‘ Ä‘á»‹nh

**CÃ´ng viá»‡c:**

**2.4.1. Xá»­ lÃ½ tá»«ng paragraph**
- Vá»›i má»—i paragraph:
  - Æ¯á»›c tÃ­nh sá»‘ tokens (1 token â‰ˆ 4 kÃ½ tá»± tiáº¿ng Viá»‡t)
  - Náº¿u < min_length (50 tokens) â†’ Bá» qua
  - Náº¿u > max_length (512 tokens) â†’ Chia nhá»
  - Náº¿u há»£p lá»‡ â†’ Táº¡o sample

**2.4.2. Chia paragraph dÃ i**
- Chia theo tá»« (word boundaries)
- Má»—i chunk ~512 tokens
- **VÃ­ dá»¥:**
  ```python
  # Paragraph 2000 tokens
  chunks = [
    "Chunk 1 (512 tokens)",
    "Chunk 2 (512 tokens)",
    "Chunk 3 (512 tokens)",
    "Chunk 4 (464 tokens)"
  ]
  ```

**2.4.3. Táº¡o sample format**
- Má»—i sample lÃ  má»™t dict:
  ```python
  {
    "text": "Ná»™i dung Ä‘oáº¡n vÄƒn...",
    "length": 512  # Sá»‘ kÃ½ tá»±
  }
  ```

**Output:** List cÃ¡c samples: `samples = [sample1, sample2, ...]`

---

### BÆ°á»›c 2.5: Chia Dataset
**Má»¥c Ä‘Ã­ch:** Chia thÃ nh train/val/test sets

**CÃ´ng viá»‡c:**

**2.5.1. Shuffle samples**
- Trá»™n ngáº«u nhiÃªn vá»›i random seed = 42 (Ä‘á»ƒ reproducible)
- **VÃ­ dá»¥:**
  ```python
  # TrÆ°á»›c shuffle: [1, 2, 3, ..., 10000]
  # Sau shuffle: [5234, 123, 7890, ..., 456]
  ```

**2.5.2. Chia theo tá»· lá»‡**
- TÃ­nh sá»‘ samples cho má»—i set:
  - Train: 80% = 8,000 samples
  - Val: 10% = 1,000 samples
  - Test: 10% = 1,000 samples
- **VÃ­ dá»¥:**
  ```python
  total = 10000
  train = shuffled[:8000]      # 80%
  val = shuffled[8000:9000]   # 10%
  test = shuffled[9000:]       # 10%
  ```

**2.5.3. LÆ°u datasets**
- `train.json`: 8,000 samples
- `val.json`: 1,000 samples
- `test.json`: 1,000 samples

**Output:** 3 files JSON chá»©a datasets

---

### BÆ°á»›c 2.6: LÆ°u Tokenizer vÃ  Metadata
**Má»¥c Ä‘Ã­ch:** LÆ°u tokenizer vÃ  thÃ´ng tin dataset

**CÃ´ng viá»‡c:**

**2.6.1. LÆ°u tokenizer**
- File: `tokenizer.pkl` (pickle format)
- Chá»©a: `word_to_id`, `id_to_word`, `vocab_size`, `special_tokens`

**2.6.2. LÆ°u metadata**
- File: `dataset_metadata.json`
- Format:
  ```json
  {
    "vocab_size": 50000,
    "train_samples": 8000,
    "val_samples": 1000,
    "test_samples": 1000,
    "total_samples": 10000
  }
  ```

## ğŸ“¤ OUTPUT
```
data/dataset/
â”œâ”€â”€ train.json              (8,000 samples)
â”œâ”€â”€ val.json                (1,000 samples)
â”œâ”€â”€ test.json               (1,000 samples)
â”œâ”€â”€ tokenizer.pkl           (Tokenizer object)
â””â”€â”€ dataset_metadata.json    (Metadata)
```

---

# ğŸ”¹ BÆ¯á»šC 3: TRAINING MODEL

## ğŸ“¥ INPUT
```
data/dataset/
â”œâ”€â”€ train.json
â”œâ”€â”€ val.json
â”œâ”€â”€ tokenizer.pkl
â””â”€â”€ dataset_metadata.json
```

## ğŸ”„ QUY TRÃŒNH CHI TIáº¾T

### BÆ°á»›c 3.1: Khá»Ÿi táº¡o Model
**Má»¥c Ä‘Ã­ch:** Táº¡o model architecture

**CÃ´ng viá»‡c:**

**3.1.1. Chá»n architecture**
- **Option 1: GPT-2 tá»« HuggingFace** (Recommended)
  - Pre-trained, dá»… sá»­ dá»¥ng
  - CÃ³ thá»ƒ fine-tune tá»« checkpoint
- **Option 2: Custom Transformer**
  - Tá»± implement tá»« Ä‘áº§u
  - Kiá»ƒm soÃ¡t hoÃ n toÃ n architecture

**3.1.2. Cáº¥u hÃ¬nh model**
```python
config = {
    "vocab_size": 50000,           # Tá»« vocabulary
    "hidden_size": 768,            # KÃ­ch thÆ°á»›c hidden layer
    "num_layers": 12,              # Sá»‘ transformer layers
    "num_heads": 12,               # Sá»‘ attention heads
    "max_seq_length": 512,         # Äá»™ dÃ i tá»‘i Ä‘a sequence
    "dropout": 0.1                 # Dropout rate
}
```

**3.1.3. Khá»Ÿi táº¡o model**
- Táº¡o model object vá»›i config trÃªn
- Move model to device (GPU náº¿u cÃ³)

**Output:** Model object sáºµn sÃ ng Ä‘á»ƒ train

---

### BÆ°á»›c 3.2: Setup Training Environment
**Má»¥c Ä‘Ã­ch:** Chuáº©n bá»‹ mÃ´i trÆ°á»ng training

**CÃ´ng viá»‡c:**

**3.2.1. Chá»n device**
- Kiá»ƒm tra GPU cÃ³ sáºµn khÃ´ng
- Náº¿u cÃ³ â†’ dÃ¹ng GPU (cuda:0)
- Náº¿u khÃ´ng â†’ dÃ¹ng CPU

**3.2.2. Táº¡o DataLoaders**
- **Train DataLoader:**
  - Batch size: 8
  - Shuffle: True
  - Num workers: 0 (Windows) hoáº·c 4+ (Linux)
- **Val DataLoader:**
  - Batch size: 8
  - Shuffle: False
  - Num workers: 0

**3.2.3. Setup Optimizer**
- Type: AdamW
- Learning rate: 5e-5
- Weight decay: 0.01
- Beta1: 0.9, Beta2: 0.999

**3.2.4. Setup Scheduler**
- Type: Cosine Annealing vá»›i Warmup
- Warmup steps: 10% tá»•ng sá»‘ steps
- T_max: Total steps - warmup steps

**Output:** Optimizer, Scheduler, DataLoaders sáºµn sÃ ng

---

### BÆ°á»›c 3.3: Training Loop
**Má»¥c Ä‘Ã­ch:** Train model qua nhiá»u epochs

**CÃ´ng viá»‡c:**

**Cho má»—i epoch (1 â†’ 3):**

**3.3.1. Training Phase**
```
For má»—i batch trong train_loader:
    1. Load batch:
       - input_ids: [batch_size, seq_length]
       - labels: [batch_size, seq_length]
    
    2. Forward pass:
       - output = model(input_ids)
       - logits = output.logits  # [batch_size, seq_length, vocab_size]
       - loss = CrossEntropyLoss(logits, labels)
    
    3. Backward pass:
       - loss.backward()
       - TÃ­nh gradients
    
    4. Gradient clipping:
       - clip_grad_norm_(max_norm=1.0)
       - TrÃ¡nh gradient explosion
    
    5. Update weights:
       - optimizer.step()
       - Cáº­p nháº­t model parameters
    
    6. Update learning rate:
       - scheduler.step()
       - Äiá»u chá»‰nh LR theo schedule
    
    7. Reset gradients:
       - optimizer.zero_grad()
    
    8. Log (má»—i 100 steps):
       - In ra: step, loss, learning_rate
```

**VÃ­ dá»¥ má»™t batch:**
```python
# Batch 1
input_ids = [[4, 5, 300, 200, ...],  # Sample 1
             [10, 20, 30, 40, ...],  # Sample 2
             ...]  # 8 samples

labels = [[5, 300, 200, 1, ...],     # Shift 1 position
          [20, 30, 40, 1, ...],
          ...]

# Forward
logits = model(input_ids)  # [8, 512, 50000]
loss = compute_loss(logits, labels)  # Scalar

# Backward & Update
loss.backward()
optimizer.step()
```

**3.3.2. Validation Phase (sau má»—i epoch)**
```
1. Set model.eval()  # Táº¯t dropout, batch norm

2. For má»—i batch trong val_loader:
    - Forward pass (khÃ´ng tÃ­nh gradient)
    - TÃ­nh validation loss
    - Accumulate loss

3. TÃ­nh average validation loss
4. TÃ­nh perplexity = exp(avg_loss)

5. So sÃ¡nh vá»›i best loss:
   - Náº¿u tá»‘t hÆ¡n â†’ LÆ°u best model
   - Cáº­p nháº­t best_loss

6. In ra: epoch, train_loss, val_loss, perplexity
```

**3.3.3. LÆ°u Checkpoint**
- Sau má»—i epoch, lÆ°u checkpoint:
  ```python
  checkpoint = {
      'epoch': epoch,
      'model_state_dict': model.state_dict(),
      'optimizer_state_dict': optimizer.state_dict(),
      'val_loss': val_loss,
      'perplexity': perplexity
  }
  torch.save(checkpoint, f'checkpoint-epoch-{epoch}/checkpoint.pt')
  ```

**Output:** 
- Checkpoints sau má»—i epoch
- Best model (dá»±a trÃªn validation loss)

---

### BÆ°á»›c 3.4: Monitor Training
**Má»¥c Ä‘Ã­ch:** Theo dÃµi quÃ¡ trÃ¬nh training

**Metrics cáº§n theo dÃµi:**

**3.4.1. Training Loss**
- Giáº£m dáº§n theo thá»i gian
- **VÃ­ dá»¥:**
  - Epoch 1: 5.2 â†’ 4.8
  - Epoch 2: 4.8 â†’ 4.3
  - Epoch 3: 4.3 â†’ 3.9

**3.4.2. Validation Loss**
- Giáº£m dáº§n, khÃ´ng tÄƒng (náº¿u tÄƒng â†’ overfitting)
- **VÃ­ dá»¥:**
  - Epoch 1: 4.5
  - Epoch 2: 4.0
  - Epoch 3: 3.7

**3.4.3. Perplexity**
- Giáº£m dáº§n (cÃ ng tháº¥p cÃ ng tá»‘t)
- **VÃ­ dá»¥:**
  - Epoch 1: 90.0
  - Epoch 2: 54.6
  - Epoch 3: 40.5

**3.4.4. Learning Rate**
- TÄƒng trong warmup phase
- Giáº£m dáº§n sau warmup (cosine schedule)

## ğŸ“¤ OUTPUT
```
models/
â”œâ”€â”€ checkpoint-epoch-1/
â”‚   â””â”€â”€ checkpoint.pt
â”œâ”€â”€ checkpoint-epoch-2/
â”‚   â””â”€â”€ checkpoint.pt
â”œâ”€â”€ checkpoint-epoch-3/
â”‚   â””â”€â”€ checkpoint.pt
â””â”€â”€ best_model/
    â””â”€â”€ model.pt          (Model vá»›i validation loss tháº¥p nháº¥t)
```

---

# ğŸ”¹ BÆ¯á»šC 4: ÄÃNH GIÃ (EVALUATION)

## ğŸ“¥ INPUT
```
models/best_model/model.pt
data/dataset/test.json
data/dataset/tokenizer.pkl
```

## ğŸ”„ QUY TRÃŒNH CHI TIáº¾T

### BÆ°á»›c 4.1: Load Model
**Má»¥c Ä‘Ã­ch:** Load model Ä‘Ã£ train Ä‘á»ƒ Ä‘Ã¡nh giÃ¡

**CÃ´ng viá»‡c:**
1. Load tokenizer tá»« `tokenizer.pkl`
2. Load model weights tá»« `model.pt`
3. Set model.eval() (táº¯t dropout, batch norm)
4. Move model to device (GPU/CPU)

**Output:** Model sáºµn sÃ ng Ä‘á»ƒ evaluate

---

### BÆ°á»›c 4.2: TÃ­nh Perplexity
**Má»¥c Ä‘Ã­ch:** Äo Ä‘á»™ "báº¥t ngá»" cá»§a model

**CÃ´ng viá»‡c:**

**4.2.1. Load test dataset**
- Äá»c `test.json`
- Táº¡o test DataLoader

**4.2.2. TÃ­nh loss trÃªn test set**
```
total_loss = 0
total_tokens = 0

For má»—i batch trong test_loader:
    - Forward pass (khÃ´ng gradient)
    - TÃ­nh loss
    - Äáº¿m sá»‘ tokens (khÃ´ng tÃ­nh padding)
    - Accumulate: total_loss += loss * num_tokens
    - Accumulate: total_tokens += num_tokens

avg_loss = total_loss / total_tokens
perplexity = exp(avg_loss)
```

**VÃ­ dá»¥:**
```python
# Batch 1: loss=3.5, tokens=4000
# Batch 2: loss=3.6, tokens=4000
# ...
# Batch 10: loss=3.4, tokens=4000

avg_loss = (3.5*4000 + 3.6*4000 + ... + 3.4*4000) / 40000
          = 3.5
perplexity = exp(3.5) = 33.1
```

**4.2.3. ÄÃ¡nh giÃ¡ káº¿t quáº£**
- **Tá»‘t:** Perplexity < 50
- **KhÃ¡:** Perplexity 50-100
- **ChÆ°a tá»‘t:** Perplexity > 100

**Output:** Perplexity score (sá»‘ thá»±c)

---

### BÆ°á»›c 4.3: Generate Text Samples
**Má»¥c Ä‘Ã­ch:** Táº¡o text máº«u Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng

**CÃ´ng viá»‡c:**

**4.3.1. Láº¥y prompts tá»« test set**
- Láº¥y 10 samples tá»« test set
- Láº¥y 100 kÃ½ tá»± Ä‘áº§u cá»§a má»—i sample lÃ m prompt
- **VÃ­ dá»¥:**
  ```python
  prompt = "Nham Kiá»u Ä‘ang Ä‘á»©ng trÃªn tÆ°á»ng thÃ nh, nhÃ¬n xuá»‘ng..."
  ```

**4.3.2. Generate text cho má»—i prompt**
```
For má»—i prompt:
    1. Encode prompt â†’ token_ids
    
    2. Initialize generated = prompt_tokens
    
    3. Loop (max 200 tokens):
       a. Forward pass: model(generated) â†’ logits
       b. Láº¥y logits cá»§a token cuá»‘i cÃ¹ng
       c. Apply temperature, top-k, top-p filtering
       d. Sample token tiáº¿p theo (multinomial)
       e. Append token vÃ o generated
       f. Náº¿u gáº·p <EOS> â†’ break
    
    4. Decode generated â†’ text
    
    5. Loáº¡i bá» prompt khá»i káº¿t quáº£
```

**VÃ­ dá»¥:**
```python
# Prompt
prompt = "Nham Kiá»u Ä‘ang Ä‘á»©ng"

# Generated (200 tokens)
generated = "Nham Kiá»u Ä‘ang Ä‘á»©ng trÃªn tÆ°á»ng thÃ nh, nhÃ¬n xuá»‘ng phÃ­a dÆ°á»›i. Háº¯n tháº¥y nhá»¯ng cÆ° dÃ¢n Ä‘ang cáº§n cÃ¹ trá»“ng trá»t. Má»™t cáº£m giÃ¡c hÃ i lÃ²ng dÃ¢ng trÃ o trong lÃ²ng háº¯n. Cuá»‘i cÃ¹ng cÅ©ng cÃ³ Ä‘Æ°á»£c lÃ£nh Ä‘á»‹a cá»§a riÃªng mÃ¬nh..."
```

**4.3.3. LÆ°u samples**
- LÆ°u prompt, generated text, original text (Ä‘á»ƒ so sÃ¡nh)

**Output:** List cÃ¡c generated samples

---

### BÆ°á»›c 4.4: ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng
**Má»¥c Ä‘Ã­ch:** ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng generated text

**TiÃªu chÃ­ Ä‘Ã¡nh giÃ¡:**

**4.4.1. Coherence (Máº¡ch láº¡c)**
- Text cÃ³ máº¡ch láº¡c, logic khÃ´ng?
- CÃ¡c cÃ¢u liÃªn káº¿t vá»›i nhau khÃ´ng?
- **VÃ­ dá»¥ tá»‘t:** "Nham Kiá»u Ä‘á»©ng trÃªn tÆ°á»ng. Háº¯n nhÃ¬n xuá»‘ng. Tháº¥y cÆ° dÃ¢n Ä‘ang lÃ m viá»‡c."
- **VÃ­ dá»¥ xáº¥u:** "Nham Kiá»u Ä‘á»©ng. MÆ°a rÆ¡i. Xe hÆ¡i cháº¡y." (khÃ´ng liÃªn quan)

**4.4.2. Relevance (LiÃªn quan)**
- Generated text cÃ³ liÃªn quan Ä‘áº¿n prompt khÃ´ng?
- **VÃ­ dá»¥ tá»‘t:** Prompt vá» "Nham Kiá»u" â†’ Generated vá» "Nham Kiá»u"
- **VÃ­ dá»¥ xáº¥u:** Prompt vá» "Nham Kiá»u" â†’ Generated vá» "HÃ´m nay trá»i Ä‘áº¹p"

**4.4.3. Repetition (Láº·p láº¡i)**
- CÃ³ láº·p láº¡i quÃ¡ nhiá»u khÃ´ng?
- **VÃ­ dá»¥ xáº¥u:** "Nham Kiá»u Nham Kiá»u Nham Kiá»u Ä‘á»©ng Ä‘á»©ng Ä‘á»©ng..."

**4.4.4. Grammar (Ngá»¯ phÃ¡p)**
- ÄÃºng ngá»¯ phÃ¡p tiáº¿ng Viá»‡t khÃ´ng?
- Dáº¥u cÃ¢u Ä‘Ãºng khÃ´ng?

**4.4.5. Length (Äá»™ dÃ i)**
- Generated text cÃ³ Ä‘á»§ dÃ i khÃ´ng? (khÃ´ng bá»‹ cáº¯t ngáº¯n)

**Output:** ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng (tá»‘t/khÃ¡/chÆ°a tá»‘t) + nháº­n xÃ©t

---

### BÆ°á»›c 4.5: LÆ°u káº¿t quáº£
**Má»¥c Ä‘Ã­ch:** LÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡

**CÃ´ng viá»‡c:**
1. Táº¡o file `evaluation_results.json`
2. Format:
```json
{
  "perplexity": 33.1,
  "test_loss": 3.5,
  "generated_samples": [
    {
      "prompt": "Nham Kiá»u Ä‘ang Ä‘á»©ng...",
      "generated": "Nham Kiá»u Ä‘ang Ä‘á»©ng trÃªn tÆ°á»ng thÃ nh...",
      "original": "Nham Kiá»u Ä‘ang Ä‘á»©ng trÃªn tÆ°á»ng thÃ nh, nhÃ¬n xuá»‘ng..."
    },
    ...
  ],
  "quality_assessment": {
    "coherence": "Tá»‘t",
    "relevance": "Tá»‘t",
    "repetition": "KhÃ´ng cÃ³",
    "grammar": "ÄÃºng"
  }
}
```

## ğŸ“¤ OUTPUT
```
results/
â””â”€â”€ evaluation_results.json
```

---

# ğŸ”¹ BÆ¯á»šC 5: INFERENCE (Sá»¬ Dá»¤NG MODEL)

## ğŸ“¥ INPUT
```
models/best_model/model.pt
data/dataset/tokenizer.pkl
User prompt (text)
```

## ğŸ”„ QUY TRÃŒNH CHI TIáº¾T

### BÆ°á»›c 5.1: Load Model
**Má»¥c Ä‘Ã­ch:** Load model Ä‘Ã£ train

**CÃ´ng viá»‡c:**
1. Load tokenizer tá»« `tokenizer.pkl`
2. Load model weights tá»« `model.pt`
3. Set model.eval()
4. Move to device

**Output:** Model sáºµn sÃ ng Ä‘á»ƒ generate

---

### BÆ°á»›c 5.2: Nháº­n Prompt
**Má»¥c Ä‘Ã­ch:** Láº¥y prompt tá»« user

**CÃ¡c cháº¿ Ä‘á»™:**

**5.2.1. Interactive Mode**
```
Loop:
    1. Print: "Nháº­p prompt (hoáº·c 'quit' Ä‘á»ƒ thoÃ¡t):"
    2. User nháº­p prompt
    3. Náº¿u 'quit' â†’ break
    4. Náº¿u prompt rá»—ng â†’ continue
    5. Generate text tá»« prompt
    6. In káº¿t quáº£
    7. Láº·p láº¡i
```

**5.2.2. Single Generation**
```
1. Prompt tá»« command line: --prompt "Nham Kiá»u Ä‘ang Ä‘á»©ng"
2. Generate text
3. In káº¿t quáº£
4. Exit
```

**5.2.3. Batch Generation**
```
1. Äá»c file prompts.txt (má»—i dÃ²ng má»™t prompt)
2. For má»—i prompt:
   - Generate text
   - LÆ°u vÃ o results
3. LÆ°u táº¥t cáº£ results vÃ o file JSON
```

**Output:** Prompt text (string)

---

### BÆ°á»›c 5.3: Encode Prompt
**Má»¥c Ä‘Ã­ch:** Chuyá»ƒn prompt thÃ nh tokens

**CÃ´ng viá»‡c:**
1. Format prompt (náº¿u cÃ³ template):
   ```python
   formatted = f"Tiáº¿p theo cÃ¢u chuyá»‡n:\n\n{prompt}\n\n"
   ```
2. Encode: `token_ids = tokenizer.encode(formatted)`
3. Convert to tensor
4. Move to device

**VÃ­ dá»¥:**
```python
prompt = "Nham Kiá»u Ä‘ang Ä‘á»©ng"
formatted = "Tiáº¿p theo cÃ¢u chuyá»‡n:\n\nNham Kiá»u Ä‘ang Ä‘á»©ng\n\n"
token_ids = [2, 4, 5, 300, 200, ...]  # [<BOS>, Nham, Kiá»u, Ä‘ang, Ä‘á»©ng, ...]
tensor = torch.tensor([token_ids]).to(device)
```

**Output:** Token tensor: `[1, seq_length]`

---

### BÆ°á»›c 5.4: Generate Text
**Má»¥c Ä‘Ã­ch:** Generate text tiáº¿p theo tá»« prompt

**CÃ´ng viá»‡c:**

**5.4.1. Initialize**
```python
generated = prompt_tokens.clone()  # Báº¯t Ä‘áº§u tá»« prompt
```

**5.4.2. Generation Loop**
```
For i in range(max_length):
    1. Forward pass:
       - output = model(generated)
       - logits = output.logits[:, -1, :]  # Láº¥y logits cá»§a token cuá»‘i
    
    2. Apply temperature:
       - logits = logits / temperature
       - Temperature cao â†’ random hÆ¡n
       - Temperature tháº¥p â†’ deterministic hÆ¡n
    
    3. Top-k filtering:
       - Láº¥y top 50 tokens cÃ³ logits cao nháº¥t
       - Set cÃ¡c tokens khÃ¡c = -inf
    
    4. Top-p (nucleus) filtering:
       - Sort tokens theo probability
       - Láº¥y tokens cÃ³ cumulative prob <= 0.9
       - Set cÃ¡c tokens khÃ¡c = -inf
    
    5. Sample:
       - probs = softmax(logits)
       - next_token = multinomial(probs)
    
    6. Append:
       - generated = concat([generated, next_token])
    
    7. Check stop condition:
       - Náº¿u next_token == <EOS> â†’ break
       - Náº¿u len(generated) >= max_length â†’ break
```

**VÃ­ dá»¥ má»™t iteration:**
```python
# Current generated: [2, 4, 5, 300, 200]  # [<BOS>, Nham, Kiá»u, Ä‘ang, Ä‘á»©ng]

# Forward
logits = model(generated)  # [1, 5, 50000]
last_logits = logits[0, -1, :]  # [50000] - logits cho token tiáº¿p theo

# Apply filters
filtered_logits = apply_topk_topp(last_logits, top_k=50, top_p=0.9)

# Sample
probs = softmax(filtered_logits / 0.8)  # temperature=0.8
next_token = sample(probs)  # VÃ­ dá»¥: 150 (token ID cá»§a "trÃªn")

# Append
generated = [2, 4, 5, 300, 200, 150]  # ThÃªm "trÃªn"
```

**5.4.3. Decode**
```python
# Generated tokens: [2, 4, 5, 300, 200, 150, ...]
# Decode
text = tokenizer.decode(generated)
# "Tiáº¿p theo cÃ¢u chuyá»‡n:\n\nNham Kiá»u Ä‘ang Ä‘á»©ng trÃªn..."
```

**5.4.4. Post-process**
- Loáº¡i bá» prompt khá»i káº¿t quáº£
- Loáº¡i bá» special tokens
- Format output

**VÃ­ dá»¥:**
```python
# Full generated
full = "Tiáº¿p theo cÃ¢u chuyá»‡n:\n\nNham Kiá»u Ä‘ang Ä‘á»©ng trÃªn tÆ°á»ng thÃ nh..."

# Remove prompt
result = "trÃªn tÆ°á»ng thÃ nh, nhÃ¬n xuá»‘ng phÃ­a dÆ°á»›i. Háº¯n tháº¥y nhá»¯ng cÆ° dÃ¢n Ä‘ang cáº§n cÃ¹ trá»“ng trá»t..."
```

**Output:** Generated text (string)

---

### BÆ°á»›c 5.5: Tráº£ vá» káº¿t quáº£
**Má»¥c Ä‘Ã­ch:** Hiá»ƒn thá»‹/lÆ°u káº¿t quáº£

**CÃ´ng viá»‡c:**

**5.5.1. Interactive Mode**
```python
print("\nğŸ“ Káº¿t quáº£:")
print("-" * 60)
print(generated_text)
print("-" * 60)
print()
```

**5.5.2. Single Generation**
```python
print(f"\nğŸ“ Generated text:")
print("-" * 60)
print(generated_text)
print("-" * 60)
```

**5.5.3. Batch Generation**
```python
results = []
for prompt in prompts:
    generated = generate(prompt)
    results.append({
        "prompt": prompt,
        "generated": generated
    })

# LÆ°u vÃ o file
with open("batch_results.json", 'w') as f:
    json.dump(results, f, ensure_ascii=False, indent=2)
```

## ğŸ“¤ OUTPUT
- **Interactive/Single:** Text in ra console
- **Batch:** File JSON chá»©a táº¥t cáº£ results

---

## ğŸ“Š Tá»”NG Káº¾T QUY TRÃŒNH

### Input â†’ Output cá»§a tá»«ng bÆ°á»›c:

```
BÆ°á»›c 1: Raw text files â†’ Cleaned paragraphs
BÆ°á»›c 2: Cleaned paragraphs â†’ Train/Val/Test datasets + Tokenizer
BÆ°á»›c 3: Datasets â†’ Trained model + Checkpoints
BÆ°á»›c 4: Trained model â†’ Evaluation metrics + Generated samples
BÆ°á»›c 5: Trained model + User prompt â†’ Generated text
```

### Thá»i gian Æ°á»›c tÃ­nh (vá»›i dataset ~10,000 samples):

- **BÆ°á»›c 1:** 5-10 phÃºt
- **BÆ°á»›c 2:** 10-15 phÃºt
- **BÆ°á»›c 3:** 2-6 giá» (tÃ¹y GPU/CPU)
- **BÆ°á»›c 4:** 5-10 phÃºt
- **BÆ°á»›c 5:** < 1 giÃ¢y má»—i generation

### Dung lÆ°á»£ng Æ°á»›c tÃ­nh:

- **Preprocessed data:** ~50-100 MB
- **Dataset:** ~100-200 MB
- **Model:** ~300-500 MB (tÃ¹y size)
- **Checkpoints:** ~1-2 GB (náº¿u lÆ°u nhiá»u)

---

**Quy trÃ¬nh nÃ y Ä‘Ã£ Ä‘Æ°á»£c mÃ´ táº£ ráº¥t chi tiáº¿t, tá»«ng bÆ°á»›c cá»¥ thá»ƒ! ğŸ¯**

